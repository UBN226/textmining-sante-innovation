# create_analysis_notebook.py
# Copie ce fichier, puis exécute `python create_analysis_notebook.py`
# Il crée le notebook analysis_oms_forbes_advanced.ipynb prêt à exécuter.

import nbformat, os, textwrap
from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell

OUT_NOTEBOOK = "analysis_oms_forbes_advanced.ipynb"

nb = new_notebook()
cells = []

cells.append(new_markdown_cell("# Analyse comparative OMS vs Forbes (Avancée)\n\n**Objectif :** mesurer dans quelle mesure les problématiques de santé mises en avant par l'OMS en Afrique sont présentes et comment elles sont traitées dans Forbes Afrique. Le notebook couvre :\n\n- extraction et définition des *topics* OMS (référence thématique),\n- scoring lexical (coverage) des articles Forbes par topic OMS,\n- scoring sémantique (similarité embeddings) des articles Forbes par topic OMS,\n- analyse de sentiment (document-level + aspect-based pour phrases liées aux topics OMS),\n- framing (rule-based),\n- extraction d'entités et réseau co-occurrence,\n- appariement OMS → Forbes (nearest neighbors),\n- tests statistiques simples.\n\n**Remarques importantes :**\n- La **temporal analysis** est volontairement exclue de ce notebook.\n- Le notebook suppose que les fichiers produits par le pipeline existent dans `/mnt/data/outputs` :\n  - `all_articles_processed.csv` (articles nettoyés),\n  - `tfidf.joblib` (optionnel),\n  - `embeddings.npy` (optionnel).\n\nExécute cellule par cellule. Les cellules contiennent des commentaires expliquant l'objectif et comment interpréter les sorties."))

cells.append(new_markdown_cell("## 0. Dépendances & installation\n\n**Objectif :** installer les paquets requis (si nécessaire). Exécute en terminal :\n\n```bash\npip install pandas numpy scikit-learn umap-learn hdbscan sentence-transformers transformers joblib spacy gensim plotly matplotlib seaborn wordcloud nltk faiss-cpu\npython -m spacy download fr_core_news_sm\npython -m spacy download en_core_web_sm\n```\n\nRemarque : le téléchargement des modèles SBERT et certains packages peuvent nécessiter internet la première fois."))

cells.append(new_code_cell(textwrap.dedent("""\
# 1) Imports & chemins - exécute cette cellule en premier\nimport os, json\nfrom collections import Counter, defaultdict\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport joblib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nOUT_DIR = '/mnt/data/outputs' if os.path.exists('/mnt/data/outputs') else 'outputs'\nCSV_PATH = os.path.join(OUT_DIR, 'all_articles_processed.csv')\nTFIDF_PATH = os.path.join(OUT_DIR, 'tfidf.joblib')\nEMB_PATH = os.path.join(OUT_DIR, 'embeddings.npy')\nRESULTS_DIR = os.path.join(OUT_DIR, 'analysis_results')\nos.makedirs(RESULTS_DIR, exist_ok=True)\nprint('Expected processed CSV at:', CSV_PATH)\n""")))

cells.append(new_code_cell(textwrap.dedent("""\
# 2) Chargement des données et séparation OMS / Forbes\nif not os.path.exists(CSV_PATH):\n    raise FileNotFoundError(f'Missing processed CSV: {CSV_PATH} — exécute le pipeline avant ce notebook')\ndf = pd.read_csv(CSV_PATH)\nprint('Dataframe shape:', df.shape)\nprint('Columns:', df.columns.tolist())\nfor c in ['source','texte_clean_bert','texte_clean_tfidf','lang','title','summary','keywords']:\n    if c not in df.columns:\n        df[c] = ''\n# Subsets\nmask_oms = df['source'].str.contains('OMS|WHO', case=False, na=False)\nmask_forbes = df['source'].str.contains('Forbes', case=False, na=False)\ndf_oms = df[mask_oms].copy()\ndf_forbes = df[mask_forbes].copy()\nprint('OMS docs:', df_oms.shape[0], 'Forbes docs:', df_forbes.shape[0])\n""")))

cells.append(new_markdown_cell("## 3) Extraction des topics OMS (référence)\nObjectif : créer des topics à partir du corpus OMS qui serviront de référence. Méthodes proposées : LDA via gensim; si gensim absent, prévoir une liste manuelle de keywords."))

cells.append(new_code_cell(textwrap.dedent("""\
# 3a) Tokenization simple pour LDA (utilise texte_clean_tfidf)\ntexts_oms = df_oms['texte_clean_tfidf'].astype(str).tolist()\ntexts_oms = [t.split() for t in texts_oms if isinstance(t, str) and t.strip()]\nprint('Number of OMS tokenized docs:', len(texts_oms))\n\n# 3b) Run gensim LDA if available\ntopics = {}\ntry:\n    import gensim\n    dictionary = gensim.corpora.Dictionary(texts_oms)\n    dictionary.filter_extremes(no_below=2, no_above=0.8, keep_n=20000)\n    corpus = [dictionary.doc2bow(text) for text in texts_oms]\n    num_topics = 8\n    lda = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=8, random_state=42)\n    for i in range(num_topics):\n        topics[i] = [word for word, prob in lda.show_topic(i, topn=15)]\n        print(f'Topic {i}:', topics[i])\n    with open(os.path.join(RESULTS_DIR, 'oms_topics.json'), 'w', encoding='utf-8') as f:\n        json.dump(topics, f, ensure_ascii=False, indent=2)\n    print('Saved OMS topics to', os.path.join(RESULTS_DIR, 'oms_topics.json'))\nexcept Exception as e:\n    print('Gensim/LDA unavailable or failed:', e)\n    # Fallback example: small manual topic lists (à adapter)\n    topics = {\n        0:['vaccin','vaccination','immunisation','dose','campagne'],\n        1:['covid','pandemi','virus','epidemie','sars'],\n        2:['sante','systeme','soins','hopital','personnel']\n    }\n    print('Using fallback topic keywords (manual).')\n""")))

cells.append(new_code_cell(textwrap.dedent("""\
# 3c) Prepare topic keyword sets (top-K tokens per topic)\nTOP_K = 30\ntopic_topk = {t: (list(v)[:TOP_K] if isinstance(v, list) else list(v)[:TOP_K]) for t,v in topics.items()}\nprint('Prepared topic keyword lists for', len(topic_topk), 'topics')\n""")))

cells.append(new_markdown_cell("## 4) Embeddings (Sentence-BERT)\nObjectif : encoder les documents pour analyses sémantiques. Charger `embeddings.npy` si présent sinon calculer avec `sentence-transformers`."))

cells.append(new_code_cell(textwrap.dedent("""\
# 4a) Load existing embeddings or compute them\nembeddings = None\nif os.path.exists(EMB_PATH):\n    try:\n        embeddings = np.load(EMB_PATH)\n        print('Loaded embeddings shape:', embeddings.shape)\n    except Exception as e:\n        print('Failed to load embeddings.npy:', e)\n\nif embeddings is None:\n    try:\n        from sentence_transformers import SentenceTransformer\n        model_name = 'all-mpnet-base-v2'\n        print('Loading SBERT model:', model_name)\n        sbert = SentenceTransformer(model_name)\n        texts_all = df['texte_clean_bert'].astype(str).tolist()\n        embeddings = sbert.encode(texts_all, show_progress_bar=True, convert_to_numpy=True)\n        np.save(EMB_PATH, embeddings)\n        print('Saved embeddings to', EMB_PATH)\n    except Exception as e:\n        print('SBERT encoding failed (offline or missing):', e)\n        embeddings = None\n\n# map indices per subset\nif embeddings is not None:\n    idx_oms = df.index[mask_oms].tolist()\n    idx_forbes = df.index[mask_forbes].tolist()\n    emb_oms = embeddings[idx_oms] if idx_oms else np.empty((0, embeddings.shape[1]))\n    emb_forbes = embeddings[idx_forbes] if idx_forbes else np.empty((0, embeddings.shape[1]))\n    print('emb_oms:', emb_oms.shape, 'emb_forbes:', emb_forbes.shape)\n""")))

cells.append(new_markdown_cell("## 5) Centroids sémantiques des topics OMS\nObjectif : pour chaque topic, calculer un centroid (moyenne des embeddings) sur les documents OMS associés au topic. Nous utilisons LDA assignments si disponibles, sinon un heuristic par mots-clés."))

cells.append(new_code_cell(textwrap.dedent("""\
# 5a) Assign OMS docs to topics (LDA assignment if available, else keywords overlap)\nnoms_topic_assign = {}\nif 'lda' in globals():\n    # If we used gensim LDA, compute doc-topic for each OMS doc\n    for i, bow in enumerate(corpus):\n        dist = lda.get_document_topics(bow)\n        if dist:\n            topic_id = max(dist, key=lambda x: x[1])[0]\n            noms_topic_assign.setdefault(topic_id, []).append(i)\n    print('Assigned OMS docs via LDA')\nelse:\n    # fallback: keyword overlap\n    oms_texts = df_oms['texte_clean_tfidf'].astype(str).tolist()\n    for i, text in enumerate(oms_texts):\n        tokens = set(text.split())\n        best_t = None; best_overlap = 0\n        for t, kws in topic_topk.items():\n            ov = len(tokens & set(kws))\n            if ov > best_overlap:\n                best_overlap = ov; best_t = t\n        if best_t is not None and best_overlap>0:\n            noms_topic_assign.setdefault(best_t, []).append(i)\n    print('Assigned OMS docs via keyword overlap (fallback)')\n\n# 5b) Compute centroids\ntopic_centroids = {}\nif embeddings is not None and len(noms_topic_assign)>0:\n    oms_global_indices = df.index[mask_oms].tolist()\n    for t, local_idxs in noms_topic_assign.items():\n        global_idxs = [oms_global_indices[i] for i in local_idxs if i < len(oms_global_indices)]\n        if not global_idxs: continue\n        vecs = embeddings[global_idxs]\n        topic_centroids[t] = vecs.mean(axis=0)\n    print('Computed centroids for topics:', list(topic_centroids.keys()))\nelse:\n    print('Embeddings or topic assignments missing; cannot compute centroids')\n""")))

cells.append(new_markdown_cell("## 6) Lexical coverage — combien d'éléments du vocabulaire d'un topic OMS apparaissent dans un article Forbes ?\nObjectif : score simple basé sur overlap tokens / topic_keywords."))

cells.append(new_code_cell(textwrap.dedent("""\
# 6a) lexical coverage function\ndef lexical_coverage(text, topic_words):\n    toks = [t for t in str(text).split() if t]\n    if not toks: return 0.0\n    overlap = sum(1 for t in toks if t in set(topic_words))\n    return overlap / len(toks)\n\n# 6b) compute coverage for Forbes articles\nlex_rows = []\nfor idx in df_forbes.index.tolist():\n    row = {'global_index': int(idx)}\n    txt = df.loc[idx, 'texte_clean_tfidf']\n    for t, kws in topic_topk.items():\n        row[f'lex_topic_{t}'] = lexical_coverage(txt, kws)\n    lex_rows.append(row)\nlex_df = pd.DataFrame(lex_rows).fillna(0)\nlex_df.to_csv(os.path.join(RESULTS_DIR, 'lexical_coverage_forbes.csv'), index=False)\nprint('Saved lexical coverage to', os.path.join(RESULTS_DIR, 'lexical_coverage_forbes.csv'))\n""")))

cells.append(new_markdown_cell("## 7) Similarité sémantique (embeddings) — mesure des distances entre articles Forbes et les centroids OMS\nObjectif : score cosinus entre embedding article Forbes et centroid topic OMS."))

cells.append(new_code_cell(textwrap.dedent("""\
# 7) compute semantic similarity if available\nif embeddings is not None and topic_centroids:\n    sim_rows = []\n    for idx in df_forbes.index.tolist():\n        emb = embeddings[int(idx)]\n        row = {'global_index': int(idx)}\n        for t, cent in topic_centroids.items():\n            sim = float(cosine_similarity(emb.reshape(1,-1), cent.reshape(1,-1))[0,0])\n            row[f'sim_topic_{t}'] = sim\n        sim_rows.append(row)\n    sim_df = pd.DataFrame(sim_rows).fillna(0)\n    sim_df.to_csv(os.path.join(RESULTS_DIR, 'semantic_similarity_forbes.csv'), index=False)\n    print('Saved semantic similarity to', os.path.join(RESULTS_DIR, 'semantic_similarity_forbes.csv'))\nelse:\n    print('Embeddings or centroids missing; semantic similarity skipped')\n""")))

cells.append(new_markdown_cell("## 8) Coverage combined (lexical OR semantic) — créer des flags de couverture\nObjectif : combiner lexical + semantic pour marquer si un article Forbes couvre un topic OMS (seuils à ajuster)."))

cells.append(new_code_cell(textwrap.dedent("""\
# 8) Combine coverage\nlex_path = os.path.join(RESULTS_DIR, 'lexical_coverage_forbes.csv')\nsim_path = os.path.join(RESULTS_DIR, 'semantic_similarity_forbes.csv')\nlex = pd.read_csv(lex_path) if os.path.exists(lex_path) else pd.DataFrame()\nsim = pd.read_csv(sim_path) if os.path.exists(sim_path) else pd.DataFrame()\nif not lex.empty:\n    cov = lex.copy()\n    if not sim.empty:\n        cov = cov.merge(sim, on='global_index', how='left')\n    # define thresholds (modifiable)\n    for t in topic_topk.keys():\n        cov[f'covered_topic_{t}'] = ((cov.get(f'lex_topic_{t}',0) > 0.08) | (cov.get(f'sim_topic_{t}',0) > 0.55)).astype(int)\n    cov.to_csv(os.path.join(RESULTS_DIR, 'coverage_combined_forbes.csv'), index=False)\n    print('Saved combined coverage flags to', os.path.join(RESULTS_DIR, 'coverage_combined_forbes.csv'))\nelse:\n    print('Lexical coverage missing; run lexical step first')\n""")))

cells.append(new_markdown_cell("## 9) Sentiment (document-level) — pipeline transformers\nObjectif : attribuer un label de sentiment (POS/NEG/NEU) pour chaque document. Utiliser avec prudence pour le français; préférence pour un modèle francophone si majoritairement FR."))

cells.append(new_code_cell(textwrap.dedent("""\
# 9) Document-level sentiment\ntry:\n    from transformers import pipeline\n    sent_pipe = pipeline('sentiment-analysis')\n    sent_labels = []\n    for idx, text in df['texte_clean_bert'].astype(str).items():\n        snippet = text[:1200]\n        res = sent_pipe(snippet)\n        sent_labels.append({'global_index': int(idx), 'label': res[0]['label'], 'score': float(res[0].get('score',0.0))})\n    sent_df = pd.DataFrame(sent_labels)\n    sent_df.to_csv(os.path.join(RESULTS_DIR, 'document_sentiment.csv'), index=False)\n    print('Saved document-level sentiment to', os.path.join(RESULTS_DIR, 'document_sentiment.csv'))\nexcept Exception as e:\n    print('Sentiment pipeline failed:', e)\n""")))

cells.append(new_markdown_cell("## 10) Aspect-based sentiment (phrase-level for topic keywords)\nObjectif : pour chaque article Forbes qui couvre un topic OMS, extraire phrases contenant topic keywords et évaluer le sentiment de ces phrases (approche simple)."))

cells.append(new_code_cell(textwrap.dedent("""\
# 10) Aspect-based sentiment\nimport re\ntry:\n    import nltk\n    nltk.download('punkt', quiet=True)\n    from nltk import sent_tokenize\nexcept Exception:\n    def sent_tokenize(x):\n        return str(x).split('. ')\n\naspect_records = []\nif os.path.exists(os.path.join(RESULTS_DIR, 'coverage_combined_forbes.csv')) and 'sent_pipe' in globals():\n    cov = pd.read_csv(os.path.join(RESULTS_DIR, 'coverage_combined_forbes.csv'))\n    for _, r in cov.iterrows():\n        gidx = int(r['global_index'])\n        art_text = df.loc[gidx, 'texte_clean_bert']\n        for t in topic_topk.keys():\n            if r.get(f'covered_topic_{t}',0) == 1:\n                kws = topic_topk[t]\n                sents = [s for s in sent_tokenize(str(art_text)) if any(re.search(r'\\b'+re.escape(k)+r'\\b', s, flags=re.I) for k in kws)]\n                if not sents:\n                    sents = sent_tokenize(str(art_text))[:3]\n                scores = []\n                for s in sents:\n                    try:\n                        out = sent_pipe(s[:1000])[0]\n                        scores.append({'label': out['label'], 'score': float(out.get('score',0.0))})\n                    except Exception:\n                        pass\n                if scores:\n                    mapping = {'POSITIVE':1,'NEGATIVE':-1}\n                    vals = [mapping.get(x['label'].upper(),0)*x['score'] for x in scores]\n                    aspect_records.append({'global_index': gidx, 'topic': t, 'aspect_sentiment': float(np.mean(vals)), 'n_sentences': len(sents)})\n    aspect_df = pd.DataFrame(aspect_records)\n    aspect_df.to_csv(os.path.join(RESULTS_DIR, 'aspect_sentiment_forbes.csv'), index=False)\n    print('Saved aspect-level sentiment to', os.path.join(RESULTS_DIR, 'aspect_sentiment_forbes.csv'))\nelse:\n    print('Either coverage file missing or sentiment pipeline not available; skip aspect-based sentiment')\n""")))

cells.append(new_markdown_cell("## 11) Framing (rule-based)\nObjectif : catégoriser le cadrage d'un article (economic / health / neutral / mixed) pour les articles Forbes couvrant un topic OMS via dictionnaires de mots-clés simples."))

cells.append(new_code_cell(textwrap.dedent("""\
# 11) Framing rule-based\nECON = set(['investissement','investir','marché','startup','business','venture','financement','investor','profit','entreprise'])\nHEALTH = set(['prévention','vaccin','vaccination','soins','sante','epidemie','mortalite','clinique','hopital','OMS','ministere'])\nframing = []\nif os.path.exists(os.path.join(RESULTS_DIR, 'coverage_combined_forbes.csv')):\n    cov = pd.read_csv(os.path.join(RESULTS_DIR, 'coverage_combined_forbes.csv'))\n    for _, r in cov.iterrows():\n        gidx = int(r['global_index'])\n        txt = set(str(df.loc[gidx,'texte_clean_bert']).lower().split())\n        for t in topic_topk.keys():\n            if r.get(f'covered_topic_{t}',0) == 1:\n                econ = len(txt & ECON)\n                heal = len(txt & HEALTH)\n                if econ > heal and econ>=1:\n                    f = 'economic'\n                elif heal > econ and heal>=1:\n                    f = 'health'\n                elif econ==0 and heal==0:\n                    f = 'neutral'\n                else:\n                    f = 'mixed'\n                framing.append({'global_index': gidx, 'topic': t, 'framing': f})\n    framing_df = pd.DataFrame(framing)\n    framing_df.to_csv(os.path.join(RESULTS_DIR, 'framing_forbes.csv'), index=False)\n    print('Saved framing to', os.path.join(RESULTS_DIR, 'framing_forbes.csv'))\nelse:\n    print('Coverage file missing; run coverage step first')\n""")))

cells.append(new_markdown_cell("## 12) Entity extraction (spaCy) et co-occurrence\nObjectif : extraire entités (ORG, PERSON, GPE) depuis les articles Forbes et produire un CSV utilisable pour construire un réseau d'acteurs."))

cells.append(new_code_cell(textwrap.dedent("""\
# 12) Entity extraction\ntry:\n    import spacy\n    nlp_fr = spacy.load('fr_core_news_sm')\n    nlp_en = spacy.load('en_core_web_sm')\nexcept Exception as e:\n    print('spaCy models not installed or failed to load:', e)\n    nlp_fr = nlp_en = None\n\nent_rows = []\nif nlp_fr is not None:\n    for idx, row in df_forbes.iterrows():\n        lang = row.get('lang','fr')\n        nlp = nlp_fr if str(lang).startswith('fr') else nlp_en\n        doc = nlp(str(row['texte_clean_bert']))\n        for ent in doc.ents:\n            if ent.label_ in ('PER','ORG','GPE','LOC','MISC','PERSON'):\n                ent_rows.append({'global_index': int(idx), 'entity': ent.text, 'label': ent.label_})\n    ent_df = pd.DataFrame(ent_rows)\n    ent_df.to_csv(os.path.join(RESULTS_DIR, 'forbes_entities.csv'), index=False)\n    print('Saved entities to', os.path.join(RESULTS_DIR, 'forbes_entities.csv'))\nelse:\n    print('spaCy not available — install the models to run NER')\n""")))

cells.append(new_markdown_cell("## 13) Appariement OMS → Forbes (nearest neighbors)\nObjectif : pour chaque document OMS, trouver les K articles Forbes les plus proches en similarité cosinus (embeddings). Permet une revue qualitative paire-à-paire."))

cells.append(new_code_cell(textwrap.dedent("""\
# 13) Nearest neighbors (brute force)\nif embeddings is not None and emb_oms.size and emb_forbes.size:\n    sims = cosine_similarity(emb_oms, emb_forbes)\n    top_k = 5\n    pairs = []\n    oms_global = df.index[mask_oms].tolist()\n    for i in range(sims.shape[0]):\n        best_idx = np.argsort(sims[i])[-top_k:][::-1]\n        for rank, j in enumerate(best_idx):\n            pairs.append({'oms_index': int(oms_global[i]), 'forbes_index': int(df.index[mask_forbes].tolist()[j]), 'rank': rank+1, 'similarity': float(sims[i, j])})\n    pairs_df = pd.DataFrame(pairs)\n    pairs_df.to_csv(os.path.join(RESULTS_DIR, 'oms_to_forbes_pairs.csv'), index=False)\n    print('Saved pairs to', os.path.join(RESULTS_DIR, 'oms_to_forbes_pairs.csv'))\nelse:\n    print('Embeddings or subsets missing; cannot compute NN pairs')\n""")))

cells.append(new_markdown_cell("## 14) Tests statistiques simples\nObjectif : exemple de test pour vérifier si la couverture d'un topic OMS par Forbes est significativement différente (illustratif). Interpréter avec prudence sur petits corpus."))

cells.append(new_code_cell(textwrap.dedent("""\
# 14) Chi-square example (illustrative)\nimport scipy.stats as stats\ncov_file = os.path.join(RESULTS_DIR, 'coverage_combined_forbes.csv')\nif os.path.exists(cov_file):\n    cov = pd.read_csv(cov_file)\n    topic = list(topic_topk.keys())[0]\n    covered = cov[f'covered_topic_{topic}'].sum()\n    not_covered = cov.shape[0] - covered\n    oms_count = len(noms_topic_assign.get(topic, [])) if 'noms_topic_assign' in globals() else 0\n    table = np.array([[covered, not_covered],[oms_count, max(1, len(df_oms)-oms_count)]])\n    chi2, p, dof, ex = stats.chi2_contingency(table)\n    print('Chi2:', chi2, 'p-value:', p)\n    with open(os.path.join(RESULTS_DIR, 'stat_tests.txt'), 'w', encoding='utf-8') as f:\n        f.write(f'Chi2 topic {topic}: {chi2}, p={p}\\n')\n    print('Saved simple stat test results to', os.path.join(RESULTS_DIR, 'stat_tests.txt'))\nelse:\n    print('Coverage file missing; run coverage steps first')\n""")))

cells.append(new_markdown_cell("## 15) Export & rapport\nLes résultats (CSV/JSON) sont sauvegardés dans `<OUT_DIR>/analysis_results` :\n- `oms_topics.json` (topics OMS),\n- `lexical_coverage_forbes.csv`, `semantic_similarity_forbes.csv`, `coverage_combined_forbes.csv`,\n- `document_sentiment.csv`, `aspect_sentiment_forbes.csv`, `framing_forbes.csv`,\n- `forbes_entities.csv`, `oms_to_forbes_pairs.csv`, `stat_tests.txt`.\n\nNext steps recommandés : ajuster seuils, inspecter manuellement paires, calibrer les modèles de sentiment pour le français, et itérer.") )

nb['cells'] = cells

with open(OUT_NOTEBOOK, 'w', encoding='utf-8') as f:
    nbformat.write(nb, f)

print('Notebook écrit:', OUT_NOTEBOOK)
print('Ouvre-le dans Jupyter et exécute cellule par cellule.')

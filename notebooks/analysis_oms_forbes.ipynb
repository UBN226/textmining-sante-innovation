{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f109bcd",
   "metadata": {},
   "source": [
    "# Analyse comparative OMS vs Forbes \n",
    "**Objectif :** mesurer dans quelle mesure les problématiques de santé mises en avant par l'OMS en Afrique sont présentes et comment elles sont traitées dans Forbes Afrique. Le notebook couvre :\n",
    "\n",
    "- extraction et définition des *topics* OMS (référence thématique),\n",
    "- scoring lexical (coverage) des articles Forbes par topic OMS,\n",
    "- scoring sémantique (similarité embeddings) des articles Forbes par topic OMS,\n",
    "- analyse de sentiment (document-level + aspect-based pour phrases liées aux topics OMS),\n",
    "- framing (rule-based),\n",
    "- extraction d'entités et réseau co-occurrence,\n",
    "- appariement OMS → Forbes (nearest neighbors),\n",
    "- tests statistiques simples.\n",
    "\n",
    "**Remarques importantes :**\n",
    "- La **temporal analysis** est volontairement exclue de ce notebook.\n",
    "- Le notebook suppose que les fichiers produits par le pipeline existent dans `/data :\n",
    "  - `all_articles_processed.csv` (articles nettoyés),\n",
    "  - `tfidf.joblib` \n",
    "  - `embeddings.npy` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473b5ffa",
   "metadata": {},
   "source": [
    "## 0. Dépendances & installation\n",
    "\n",
    "**Objectif :** installer les paquets requis (si nécessaire). Exécute en terminal :\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy scikit-learn umap-learn hdbscan sentence-transformers transformers joblib spacy gensim plotly matplotlib seaborn wordcloud nltk faiss-cpu\n",
    "python -m spacy download fr_core_news_sm\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b3a480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected processed CSV at: outputs\\all_articles_processed.csv\n"
     ]
    }
   ],
   "source": [
    "# 1) Imports & chemins - exécute cette cellule en premier\n",
    "import os, json\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "OUT_DIR = '../data/outputs' if os.path.exists('../data/outputs') else 'outputs'\n",
    "CSV_PATH = os.path.join(OUT_DIR, 'all_articles_processed.csv')\n",
    "TFIDF_PATH = os.path.join(OUT_DIR, 'tfidf.joblib')\n",
    "EMB_PATH = os.path.join(OUT_DIR, 'embeddings.npy')\n",
    "RESULTS_DIR = os.path.join(OUT_DIR, 'analysis_results')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "print('Expected processed CSV at:', CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6a5ddc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(461, 44)\n",
      "['source', 'titre', 'date', 'lien', 'texte', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18', 'Unnamed: 19', 'Unnamed: 20', 'Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24', 'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27', 'Unnamed: 28', 'Unnamed: 29', 'Unnamed: 30', 'Unnamed: 31', 'Unnamed: 32', 'Unnamed: 33', 'Unnamed: 34', 'Unnamed: 35', 'Unnamed: 36', 'Unnamed: 37', 'Unnamed: 38', 'Unnamed: 39', 'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42', 'Unnamed: 43']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>titre</th>\n",
       "      <th>date</th>\n",
       "      <th>lien</th>\n",
       "      <th>texte</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 34</th>\n",
       "      <th>Unnamed: 35</th>\n",
       "      <th>Unnamed: 36</th>\n",
       "      <th>Unnamed: 37</th>\n",
       "      <th>Unnamed: 38</th>\n",
       "      <th>Unnamed: 39</th>\n",
       "      <th>Unnamed: 40</th>\n",
       "      <th>Unnamed: 41</th>\n",
       "      <th>Unnamed: 42</th>\n",
       "      <th>Unnamed: 43</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OMS</td>\n",
       "      <td>Soins, compassion et guérison du plus jeune pa...</td>\n",
       "      <td>2025-11-05T12:00:00Z</td>\n",
       "      <td>https://www.afro.who.int/fr/countries/democrat...</td>\n",
       "      <td>Soins, compassion et guérison du plus jeune pa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OMS</td>\n",
       "      <td>Comprendre la prévalence, les risques et les m...</td>\n",
       "      <td>2025-11-03T12:00:00Z</td>\n",
       "      <td>https://www.afro.who.int/fr/countries/senegal/...</td>\n",
       "      <td>Comprendre la prévalence, les risques et les m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OMS</td>\n",
       "      <td>Les centres de prise en charge au cœur de la l...</td>\n",
       "      <td>2025-10-31T12:00:00Z</td>\n",
       "      <td>https://www.afro.who.int/fr/countries/burundi/...</td>\n",
       "      <td>Les centres de prise en charge au cœur de la l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OMS</td>\n",
       "      <td>L’épidémie de choléra en recul au Congo</td>\n",
       "      <td>2025-10-21T12:00:00Z</td>\n",
       "      <td>https://www.afro.who.int/fr/countries/congo/ne...</td>\n",
       "      <td>L’épidémie de choléra en recul au Congo\\n21 oc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OMS</td>\n",
       "      <td>RDC : le deuil coutumier de 40 jours aide à fr...</td>\n",
       "      <td>2025-10-18T12:00:00Z</td>\n",
       "      <td>https://www.afro.who.int/fr/countries/democrat...</td>\n",
       "      <td>RDC : le deuil coutumier de 40 jours aide à fr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  source                                              titre  \\\n",
       "0    OMS  Soins, compassion et guérison du plus jeune pa...   \n",
       "1    OMS  Comprendre la prévalence, les risques et les m...   \n",
       "2    OMS  Les centres de prise en charge au cœur de la l...   \n",
       "3    OMS            L’épidémie de choléra en recul au Congo   \n",
       "4    OMS  RDC : le deuil coutumier de 40 jours aide à fr...   \n",
       "\n",
       "                   date                                               lien  \\\n",
       "0  2025-11-05T12:00:00Z  https://www.afro.who.int/fr/countries/democrat...   \n",
       "1  2025-11-03T12:00:00Z  https://www.afro.who.int/fr/countries/senegal/...   \n",
       "2  2025-10-31T12:00:00Z  https://www.afro.who.int/fr/countries/burundi/...   \n",
       "3  2025-10-21T12:00:00Z  https://www.afro.who.int/fr/countries/congo/ne...   \n",
       "4  2025-10-18T12:00:00Z  https://www.afro.who.int/fr/countries/democrat...   \n",
       "\n",
       "                                               texte Unnamed: 5 Unnamed: 6  \\\n",
       "0  Soins, compassion et guérison du plus jeune pa...        NaN        NaN   \n",
       "1  Comprendre la prévalence, les risques et les m...        NaN        NaN   \n",
       "2  Les centres de prise en charge au cœur de la l...        NaN        NaN   \n",
       "3  L’épidémie de choléra en recul au Congo\\n21 oc...        NaN        NaN   \n",
       "4  RDC : le deuil coutumier de 40 jours aide à fr...        NaN        NaN   \n",
       "\n",
       "  Unnamed: 7 Unnamed: 8 Unnamed: 9  ... Unnamed: 34 Unnamed: 35 Unnamed: 36  \\\n",
       "0        NaN        NaN        NaN  ...         NaN         NaN         NaN   \n",
       "1        NaN        NaN        NaN  ...         NaN         NaN         NaN   \n",
       "2        NaN        NaN        NaN  ...         NaN         NaN         NaN   \n",
       "3        NaN        NaN        NaN  ...         NaN         NaN         NaN   \n",
       "4        NaN        NaN        NaN  ...         NaN         NaN         NaN   \n",
       "\n",
       "  Unnamed: 37 Unnamed: 38 Unnamed: 39 Unnamed: 40 Unnamed: 41 Unnamed: 42  \\\n",
       "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "  Unnamed: 43  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Fusion des fichiers\n",
    "\n",
    "oms = pd.read_csv(\"../outputs/oms_articles.csv\")      \n",
    "forbes = pd.read_csv(\"../outputs/forbes_articles_2.csv\")\n",
    "\n",
    "oms['source'] = 'OMS'\n",
    "forbes['source'] = 'Forbes'\n",
    "df = pd.concat([oms, forbes], ignore_index=True)\n",
    "\n",
    "# aperçu\n",
    "print(df.shape)\n",
    "print(df.columns.tolist())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fb721e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:, [0, 1, 2, 3, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4bd2385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>titre</th>\n",
       "      <th>date</th>\n",
       "      <th>lien</th>\n",
       "      <th>texte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OMS</td>\n",
       "      <td>Soins, compassion et guérison du plus jeune pa...</td>\n",
       "      <td>2025-11-05T12:00:00Z</td>\n",
       "      <td>https://www.afro.who.int/fr/countries/democrat...</td>\n",
       "      <td>Soins, compassion et guérison du plus jeune pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OMS</td>\n",
       "      <td>Comprendre la prévalence, les risques et les m...</td>\n",
       "      <td>2025-11-03T12:00:00Z</td>\n",
       "      <td>https://www.afro.who.int/fr/countries/senegal/...</td>\n",
       "      <td>Comprendre la prévalence, les risques et les m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OMS</td>\n",
       "      <td>Les centres de prise en charge au cœur de la l...</td>\n",
       "      <td>2025-10-31T12:00:00Z</td>\n",
       "      <td>https://www.afro.who.int/fr/countries/burundi/...</td>\n",
       "      <td>Les centres de prise en charge au cœur de la l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OMS</td>\n",
       "      <td>L’épidémie de choléra en recul au Congo</td>\n",
       "      <td>2025-10-21T12:00:00Z</td>\n",
       "      <td>https://www.afro.who.int/fr/countries/congo/ne...</td>\n",
       "      <td>L’épidémie de choléra en recul au Congo\\n21 oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OMS</td>\n",
       "      <td>RDC : le deuil coutumier de 40 jours aide à fr...</td>\n",
       "      <td>2025-10-18T12:00:00Z</td>\n",
       "      <td>https://www.afro.who.int/fr/countries/democrat...</td>\n",
       "      <td>RDC : le deuil coutumier de 40 jours aide à fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source                                              titre  \\\n",
       "0    OMS  Soins, compassion et guérison du plus jeune pa...   \n",
       "1    OMS  Comprendre la prévalence, les risques et les m...   \n",
       "2    OMS  Les centres de prise en charge au cœur de la l...   \n",
       "3    OMS            L’épidémie de choléra en recul au Congo   \n",
       "4    OMS  RDC : le deuil coutumier de 40 jours aide à fr...   \n",
       "\n",
       "                   date                                               lien  \\\n",
       "0  2025-11-05T12:00:00Z  https://www.afro.who.int/fr/countries/democrat...   \n",
       "1  2025-11-03T12:00:00Z  https://www.afro.who.int/fr/countries/senegal/...   \n",
       "2  2025-10-31T12:00:00Z  https://www.afro.who.int/fr/countries/burundi/...   \n",
       "3  2025-10-21T12:00:00Z  https://www.afro.who.int/fr/countries/congo/ne...   \n",
       "4  2025-10-18T12:00:00Z  https://www.afro.who.int/fr/countries/democrat...   \n",
       "\n",
       "                                               texte  \n",
       "0  Soins, compassion et guérison du plus jeune pa...  \n",
       "1  Comprendre la prévalence, les risques et les m...  \n",
       "2  Les centres de prise en charge au cœur de la l...  \n",
       "3  L’épidémie de choléra en recul au Congo\\n21 oc...  \n",
       "4  RDC : le deuil coutumier de 40 jours aide à fr...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7701109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Nettoyage de base (HTML, espaces, encodage, dates)\n",
    "\n",
    "# === Nettoyage léger (pour BERT) et prétraitement pour TF-IDF ===\n",
    "from langdetect import detect\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp_fr = spacy.load(\"fr_core_news_sm\", disable=[\"parser\",\"ner\"])\n",
    "nlp_en = spacy.load(\"en_core_web_sm\", disable=[\"parser\",\"ner\"])\n",
    "\n",
    "def clean_html_only(text):\n",
    "    \"\"\"Nettoyage léger : retire script/style/URLs, conserve phrases intactes.\"\"\"\n",
    "    soup = BeautifulSoup(str(text), \"html.parser\")\n",
    "    for s in soup([\"script\",\"style\"]):\n",
    "        s.decompose()\n",
    "    out = soup.get_text(separator=\" \")\n",
    "    out = re.sub(r\"https?://\\S+|www\\.\\S+|\\S+@\\S+\", \" \", out)\n",
    "    out = out.replace(\"’\", \"'\").replace(\"‘\",\"'\")\n",
    "    out = re.sub(r\"\\s+\", \" \", out).strip()\n",
    "    txt = re.sub(r'javascript.*disabled.*', ' ', text, flags=re.I)\n",
    "    txt = re.sub(r'cookie.*', ' ', text, flags=re.I)\n",
    "    return out\n",
    "\n",
    "def preprocess_for_tfidf(text, lang_hint='fr', min_tok=2):\n",
    "    \"\"\"Nettoyage plus agressif pour TF-IDF / LDA : lowercase, remove stopwords, lemmatisation.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text or \"\")\n",
    "    text = clean_html_only(text)\n",
    "    text = text.lower()\n",
    "    # keep letters & accents & apostrophes and spaces\n",
    "    text = re.sub(r\"[^a-z0-9àâäçéèêëîïôöùûüÿœæ'\\s-]\", \" \", text)\n",
    "    doc = (nlp_fr if str(lang_hint).startswith(\"fr\") else nlp_en)(text)\n",
    "    toks = []\n",
    "    for t in doc:\n",
    "        if t.is_stop or t.is_punct or t.is_space or t.like_num:\n",
    "            continue\n",
    "        lemma = t.lemma_.lower().strip()\n",
    "        if len(lemma) < min_tok:\n",
    "            continue\n",
    "        toks.append(lemma)\n",
    "    return \" \".join(toks)\n",
    "\n",
    "# detection de langue\n",
    "\n",
    "def safe_detect(s):\n",
    "    try: return detect(s)\n",
    "    except: return 'unknown'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "204c7f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texte</th>\n",
       "      <th>texte_clean_bert</th>\n",
       "      <th>texte_clean_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Soins, compassion et guérison du plus jeune pa...</td>\n",
       "      <td>Soins, compassion et guérison du plus jeune pa...</td>\n",
       "      <td>soin compassion guérison jeune patient épidémi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comprendre la prévalence, les risques et les m...</td>\n",
       "      <td>Comprendre la prévalence, les risques et les m...</td>\n",
       "      <td>comprendre prévalence risque mécanisme prévent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Les centres de prise en charge au cœur de la l...</td>\n",
       "      <td>Les centres de prise en charge au cœur de la l...</td>\n",
       "      <td>centre prise charge cœur lutte contre mpo buru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texte  \\\n",
       "0  Soins, compassion et guérison du plus jeune pa...   \n",
       "1  Comprendre la prévalence, les risques et les m...   \n",
       "2  Les centres de prise en charge au cœur de la l...   \n",
       "\n",
       "                                    texte_clean_bert  \\\n",
       "0  Soins, compassion et guérison du plus jeune pa...   \n",
       "1  Comprendre la prévalence, les risques et les m...   \n",
       "2  Les centres de prise en charge au cœur de la l...   \n",
       "\n",
       "                                   texte_clean_tfidf  \n",
       "0  soin compassion guérison jeune patient épidémi...  \n",
       "1  comprendre prévalence risque mécanisme prévent...  \n",
       "2  centre prise charge cœur lutte contre mpo buru...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df.copy()\n",
    "\n",
    "\n",
    "df_clean['texte_clean_bert'] = df['texte'].astype(str).apply(clean_html_only)\n",
    "df_clean['lang'] = df_clean['texte_clean_bert'].apply(lambda s: safe_detect(s) if s.strip() else 'unknown')\n",
    "df_clean['texte_clean_tfidf'] = df_clean.apply(lambda r: preprocess_for_tfidf(r['texte'], lang_hint=r['lang']), axis=1)\n",
    "\n",
    "# vérifier quelques exemples\n",
    "df_clean[['texte','texte_clean_bert','texte_clean_tfidf']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65f75fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doublons exacts sur le texte\n",
    "df_clean = df_clean.drop_duplicates(subset=['texte']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e5b42ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMS docs: 59 Forbes docs: 151\n"
     ]
    }
   ],
   "source": [
    "# 3) Chargement des données et séparation OMS / Forbes\n",
    "\n",
    "\n",
    "for c in ['source','texte_clean_bert','texte_clean_tfidf','lang','titre','summary','keywords']:\n",
    "    if c not in df_clean.columns:\n",
    "        df_clean[c] = ''\n",
    "\n",
    "mask_oms = df_clean['source'].str.contains('OMS|WHO', case=False, na=False)\n",
    "mask_forbes = df_clean['source'].str.contains('Forbes', case=False, na=False)\n",
    "df_oms = df_clean[mask_oms].copy()\n",
    "df_forbes = df_clean[mask_forbes].copy()\n",
    "print('OMS docs:', df_oms.shape[0], 'Forbes docs:', df_forbes.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d9fc5f",
   "metadata": {},
   "source": [
    "## 3) Extraction des topics OMS (référence)\n",
    "Objectif : créer des topics à partir du corpus OMS qui serviront de référence. Méthodes proposées : LDA via gensim; si gensim absent, prévoir une liste manuelle de keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "902fb74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OMS tokenized docs: 59\n",
      "Topic 0: ['enfant', 'charge', 'soin', 'service', 'vaccination', 'personne', 'centre', 'région', 'accès', 'mobile', 'contre', 'zone', 'jeune', 'cas', 'effort']\n",
      "Topic 1: ['traitement', 'soin', 'maladie', 'ebola', 'centre', 'patient', 'national', 'dépistage', 'service', 'cancer', 'mettre', 'vih', 'contre', 'femme', 'lutte']\n",
      "Topic 2: ['ambulance', 'enfant', 'jour', 'communautaire', 'cas', 'charge', 'sanitaire', 'contact', 'il', 'congo', '-t', 'agent', 'prise', 'district', 'prendre']\n",
      "Topic 3: ['femme', 'maternel', 'contre', 'communauté', 'enfant', 'maladie', 'pays', 'soin', 'an', 'communautaire', 'campagne', 'décès', 'population', 'sanitaire', 'accouchement']\n",
      "Topic 4: ['soin', 'centre', 'charge', 'eau', 'service', 'choléra', 'accès', 'prise', 'cas', 'décès', 'former', 'communauté', 'femme', 'épidémie', 'traitement']\n",
      "Topic 5: ['tuberculose', 'traitement', 'diagnostic', 'maladie', 'contre', 'pays', 'patient', 'lutte', 'cas', 'radio', 'mpo', 'laboratoire', 'dépistage', 'personne', 'africain']\n",
      "Topic 6: ['patient', 'moto', 'mpo', 'ambulance', 'charge', 'centre', 'prendre', 'sanitaire', 'district', 'permettre', 'personne', 'mettre', 'vie', 'prise', 'mieux']\n",
      "Topic 7: ['épidémie', 'cas', 'maladie', 'renforcer', 'urgence', 'équipe', 'sanitaire', 'rdc', 'surveillance', 'communauté', 'communautaire', 'personne', 'appui', 'choléra', 'pays']\n",
      "Saved OMS topics to outputs\\analysis_results\\oms_topics.json\n"
     ]
    }
   ],
   "source": [
    "# 3a) Tokenization simple pour LDA (utilise texte_clean_tfidf)\n",
    "texts_oms = df_oms['texte_clean_tfidf'].astype(str).tolist()\n",
    "texts_oms = [t.split() for t in texts_oms if isinstance(t, str) and t.strip()]\n",
    "print('Number of OMS tokenized docs:', len(texts_oms))\n",
    "\n",
    "# 3b) Run gensim LDA if available\n",
    "topics = {}\n",
    "try:\n",
    "    import gensim\n",
    "    dictionary = gensim.corpora.Dictionary(texts_oms)\n",
    "    dictionary.filter_extremes(no_below=2, no_above=0.8, keep_n=20000)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts_oms]\n",
    "    num_topics = 8\n",
    "    lda = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=8, random_state=42)\n",
    "    for i in range(num_topics):\n",
    "        topics[i] = [word for word, prob in lda.show_topic(i, topn=15)]\n",
    "        print(f'Topic {i}:', topics[i])\n",
    "    with open(os.path.join(RESULTS_DIR, 'oms_topics.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(topics, f, ensure_ascii=False, indent=2)\n",
    "    print('Saved OMS topics to', os.path.join(RESULTS_DIR, 'oms_topics.json'))\n",
    "except Exception as e:\n",
    "    print('Gensim/LDA unavailable or failed:', e)\n",
    "    # Fallback example: small manual topic lists (à adapter)\n",
    "    topics = {\n",
    "        0:['vaccin','vaccination','immunisation','dose','campagne'],\n",
    "        1:['covid','pandemi','virus','epidemie','sars'],\n",
    "        2:['sante','systeme','soins','hopital','personnel']\n",
    "    }\n",
    "    print('Using fallback topic keywords (manual).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08329c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared topic keyword lists for 8 topics\n"
     ]
    }
   ],
   "source": [
    "# 3c) Prepare topic keyword sets (top-K tokens per topic)\n",
    "TOP_K = 30\n",
    "topic_topk = {t: (list(v)[:TOP_K] if isinstance(v, list) else list(v)[:TOP_K]) for t,v in topics.items()}\n",
    "print('Prepared topic keyword lists for', len(topic_topk), 'topics')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817f68e",
   "metadata": {},
   "source": [
    "## 4) Embeddings (Sentence-BERT)\n",
    "Objectif : encoder les documents pour analyses sémantiques. Charger `embeddings.npy` si présent sinon calculer avec `sentence-transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80b4e930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Mes_dossiers\\IDSI\\M2\\Course\\Text_Mining\\textmining-sante-innovation\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SBERT model: all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 7/7 [00:58<00:00,  8.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings to outputs\\embeddings.npy\n",
      "emb_oms: (59, 768) emb_forbes: (151, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 4a) Load existing embeddings or compute them\n",
    "embeddings = None\n",
    "if os.path.exists(EMB_PATH):\n",
    "    try:\n",
    "        embeddings = np.load(EMB_PATH)\n",
    "        print('Loaded embeddings shape:', embeddings.shape)\n",
    "    except Exception as e:\n",
    "        print('Failed to load embeddings.npy:', e)\n",
    "\n",
    "if embeddings is None:\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        model_name = 'all-mpnet-base-v2'\n",
    "        print('Loading SBERT model:', model_name)\n",
    "        sbert = SentenceTransformer(model_name)\n",
    "        texts_all = df_clean['texte_clean_bert'].astype(str).tolist()\n",
    "        embeddings = sbert.encode(texts_all, show_progress_bar=True, convert_to_numpy=True)\n",
    "        np.save(EMB_PATH, embeddings)\n",
    "        print('Saved embeddings to', EMB_PATH)\n",
    "    except Exception as e:\n",
    "        print('SBERT encoding failed (offline or missing):', e)\n",
    "        embeddings = None\n",
    "\n",
    "# map indices per subset\n",
    "if embeddings is not None:\n",
    "    idx_oms = df_clean.index[mask_oms].tolist()\n",
    "    idx_forbes = df_clean.index[mask_forbes].tolist()\n",
    "    emb_oms = embeddings[idx_oms] if idx_oms else np.empty((0, embeddings.shape[1]))\n",
    "    emb_forbes = embeddings[idx_forbes] if idx_forbes else np.empty((0, embeddings.shape[1]))\n",
    "    print('emb_oms:', emb_oms.shape, 'emb_forbes:', emb_forbes.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c276ff",
   "metadata": {},
   "source": [
    "## 5) Centroids sémantiques des topics OMS\n",
    "Objectif : pour chaque topic, calculer un centroid (moyenne des embeddings) sur les documents OMS associés au topic. Nous utilisons LDA assignments si disponibles, sinon un heuristic par mots-clés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "032a86b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned OMS docs via LDA\n",
      "Computed centroids for topics: [1, 5, 6, 4, 2, 0, 3, 7]\n"
     ]
    }
   ],
   "source": [
    "# 5a) Assign OMS docs to topics (LDA assignment if available, else keywords overlap)\n",
    "noms_topic_assign = {}\n",
    "if 'lda' in globals():\n",
    "    # If we used gensim LDA, compute doc-topic for each OMS doc\n",
    "    for i, bow in enumerate(corpus):\n",
    "        dist = lda.get_document_topics(bow)\n",
    "        if dist:\n",
    "            topic_id = max(dist, key=lambda x: x[1])[0]\n",
    "            noms_topic_assign.setdefault(topic_id, []).append(i)\n",
    "    print('Assigned OMS docs via LDA')\n",
    "else:\n",
    "    # fallback: keyword overlap\n",
    "    oms_texts = df_oms['texte_clean_tfidf'].astype(str).tolist()\n",
    "    for i, text in enumerate(oms_texts):\n",
    "        tokens = set(text.split())\n",
    "        best_t = None; best_overlap = 0\n",
    "        for t, kws in topic_topk.items():\n",
    "            ov = len(tokens & set(kws))\n",
    "            if ov > best_overlap:\n",
    "                best_overlap = ov; best_t = t\n",
    "        if best_t is not None and best_overlap>0:\n",
    "            noms_topic_assign.setdefault(best_t, []).append(i)\n",
    "    print('Assigned OMS docs via keyword overlap (fallback)')\n",
    "\n",
    "# 5b) Compute centroids\n",
    "topic_centroids = {}\n",
    "if embeddings is not None and len(noms_topic_assign)>0:\n",
    "    oms_global_indices = df_clean.index[mask_oms].tolist()\n",
    "    for t, local_idxs in noms_topic_assign.items():\n",
    "        global_idxs = [oms_global_indices[i] for i in local_idxs if i < len(oms_global_indices)]\n",
    "        if not global_idxs: continue\n",
    "        vecs = embeddings[global_idxs]\n",
    "        topic_centroids[t] = vecs.mean(axis=0)\n",
    "    print('Computed centroids for topics:', list(topic_centroids.keys()))\n",
    "else:\n",
    "    print('Embeddings or topic assignments missing; cannot compute centroids')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abc5e9e",
   "metadata": {},
   "source": [
    "## 6) Lexical coverage — combien d'éléments du vocabulaire d'un topic OMS apparaissent dans un article Forbes ?\n",
    "Objectif : score simple basé sur overlap tokens / topic_keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9eff87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved lexical coverage to outputs\\analysis_results\\lexical_coverage_forbes.csv\n"
     ]
    }
   ],
   "source": [
    "# 6a) lexical coverage function\n",
    "def lexical_coverage(text, topic_words):\n",
    "    toks = [t for t in str(text).split() if t]\n",
    "    if not toks: return 0.0\n",
    "    overlap = sum(1 for t in toks if t in set(topic_words))\n",
    "    return overlap / len(toks)\n",
    "\n",
    "# 6b) compute coverage for Forbes articles\n",
    "lex_rows = []\n",
    "for idx in df_forbes.index.tolist():\n",
    "    row = {'global_index': int(idx)}\n",
    "    txt = df_clean.loc[idx, 'texte_clean_tfidf']\n",
    "    for t, kws in topic_topk.items():\n",
    "        row[f'lex_topic_{t}'] = lexical_coverage(txt, kws)\n",
    "    lex_rows.append(row)\n",
    "lex_df = pd.DataFrame(lex_rows).fillna(0)\n",
    "lex_df.to_csv(os.path.join(RESULTS_DIR, 'lexical_coverage_forbes.csv'), index=False)\n",
    "print('Saved lexical coverage to', os.path.join(RESULTS_DIR, 'lexical_coverage_forbes.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0491edf2",
   "metadata": {},
   "source": [
    "## 7) Similarité sémantique (embeddings) — mesure des distances entre articles Forbes et les centroids OMS\n",
    "Objectif : score cosinus entre embedding article Forbes et centroid topic OMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b02f433a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved semantic similarity to outputs\\analysis_results\\semantic_similarity_forbes.csv\n"
     ]
    }
   ],
   "source": [
    "# 7) compute semantic similarity if available\n",
    "if embeddings is not None and topic_centroids:\n",
    "    sim_rows = []\n",
    "    for idx in df_forbes.index.tolist():\n",
    "        emb = embeddings[int(idx)]\n",
    "        row = {'global_index': int(idx)}\n",
    "        for t, cent in topic_centroids.items():\n",
    "            sim = float(cosine_similarity(emb.reshape(1,-1), cent.reshape(1,-1))[0,0])\n",
    "            row[f'sim_topic_{t}'] = sim\n",
    "        sim_rows.append(row)\n",
    "    sim_df = pd.DataFrame(sim_rows).fillna(0)\n",
    "    sim_df.to_csv(os.path.join(RESULTS_DIR, 'semantic_similarity_forbes.csv'), index=False)\n",
    "    print('Saved semantic similarity to', os.path.join(RESULTS_DIR, 'semantic_similarity_forbes.csv'))\n",
    "else:\n",
    "    print('Embeddings or centroids missing; semantic similarity skipped')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a9191",
   "metadata": {},
   "source": [
    "## 8) Coverage combined (lexical OR semantic) — créer des flags de couverture\n",
    "Objectif : combiner lexical + semantic pour marquer si un article Forbes couvre un topic OMS (seuils à ajuster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8054df39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined coverage flags to outputs\\analysis_results\\coverage_combined_forbes.csv\n"
     ]
    }
   ],
   "source": [
    "# 8) Combine coverage\n",
    "lex_path = os.path.join(RESULTS_DIR, 'lexical_coverage_forbes.csv')\n",
    "sim_path = os.path.join(RESULTS_DIR, 'semantic_similarity_forbes.csv')\n",
    "lex = pd.read_csv(lex_path) if os.path.exists(lex_path) else pd.DataFrame()\n",
    "sim = pd.read_csv(sim_path) if os.path.exists(sim_path) else pd.DataFrame()\n",
    "if not lex.empty:\n",
    "    cov = lex.copy()\n",
    "    if not sim.empty:\n",
    "        cov = cov.merge(sim, on='global_index', how='left')\n",
    "    # define thresholds (modifiable)\n",
    "    for t in topic_topk.keys():\n",
    "        cov[f'covered_topic_{t}'] = ((cov.get(f'lex_topic_{t}',0) > 0.08) | (cov.get(f'sim_topic_{t}',0) > 0.55)).astype(int)\n",
    "    cov.to_csv(os.path.join(RESULTS_DIR, 'coverage_combined_forbes.csv'), index=False)\n",
    "    print('Saved combined coverage flags to', os.path.join(RESULTS_DIR, 'coverage_combined_forbes.csv'))\n",
    "else:\n",
    "    print('Lexical coverage missing; run lexical step first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd5b51c",
   "metadata": {},
   "source": [
    "## 9) Sentiment (document-level) — pipeline transformers\n",
    "Objectif : attribuer un label de sentiment (POS/NEG/NEU) pour chaque document. Utiliser avec prudence pour le français; préférence pour un modèle francophone si majoritairement FR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b26d2473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Sentiment batches: 100%|██████████| 58/58 [00:48<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sentiment-annotated CSV to outputs\\all_with_sentiment.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 9) Document-level sentiment\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# 4) Choix du modèle\n",
    "# Option A : modèle multilingue utile si corpus FR+EN -> nlptown/bert-base-multilingual-uncased-sentiment (1-5 stars)\n",
    "# Option B : modèle francophone si corpus majoritairement FR -> 'nlptown/bert-base-multilingual-uncased-sentiment' is still ok\n",
    "# Option C : modèle anglais si corpus majoritairement EN -> 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "MODEL = 'nlptown/bert-base-multilingual-uncased-sentiment'  # recommandé pour FR+EN simple\n",
    "\n",
    "# 5) Créer pipeline explicitement (device = 0 for GPU, -1 for CPU)\n",
    "# If you have GPU available, set device=0; otherwise leave device=-1\n",
    "sent_pipe = pipeline('sentiment-analysis', model=MODEL, tokenizer=MODEL, device=-1)\n",
    "\n",
    "# 6) Utility: map model outputs to POS/NEU/NEG\n",
    "# Note: nlptown returns labels like \"1 star\", \"2 stars\"... we map them:\n",
    "def map_multilang_label(label):\n",
    "    # examples: '1 star', '2 stars', ..., '5 stars'\n",
    "    if isinstance(label, str) and 'star' in label:\n",
    "        n = int(label.split()[0])\n",
    "        if n <= 2:\n",
    "            return 'NEG'\n",
    "        elif n == 3:\n",
    "            return 'NEU'\n",
    "        else:\n",
    "            return 'POS'\n",
    "    # fallback for English-style labels\n",
    "    if label.upper().startswith('NEG'):\n",
    "        return 'NEG'\n",
    "    if label.upper().startswith('POS'):\n",
    "        return 'POS'\n",
    "    return 'NEU'\n",
    "\n",
    "# 7) Batch inference (safe: truncate long texts)\n",
    "BATCH = 8\n",
    "results = []\n",
    "for i in tqdm(range(0, len(df), BATCH), desc='Sentiment batches'):\n",
    "    batch_texts = df_clean['texte_clean_bert'].iloc[i:i+BATCH].tolist()\n",
    "    # truncate each text to ~1000 chars (adjust) to avoid pipeline failures/long times\n",
    "    batch_texts_trunc = [t[:1200] for t in batch_texts]\n",
    "    try:\n",
    "        outs = sent_pipe(batch_texts_trunc)\n",
    "    except Exception as e:\n",
    "        # fallback: try per-item to isolate problematic examples\n",
    "        outs = []\n",
    "        for t in batch_texts_trunc:\n",
    "            try:\n",
    "                outs.append(sent_pipe(t)[0])\n",
    "            except Exception as ee:\n",
    "                outs.append({'label': 'NEU', 'score': 0.0})\n",
    "    # normalize outputs\n",
    "    for out in outs:\n",
    "        label = out.get('label', '')\n",
    "        score = float(out.get('score', 0.0))\n",
    "        norm_label = map_multilang_label(label)\n",
    "        results.append({'label_raw': label, 'label': norm_label, 'score': score})\n",
    "\n",
    "# 8) Attacher résultats au dataframe et sauvegarder\n",
    "res_df = pd.DataFrame(results)\n",
    "res_df.index = df_clean.index  # align indices\n",
    "df_clean['sent_label_raw'] = res_df['label_raw']\n",
    "df_clean['sent_label'] = res_df['label']\n",
    "df_clean['sent_score'] = res_df['score']\n",
    "\n",
    "OUT_CSV = os.path.join(OUT_DIR, 'all_with_sentiment.csv')\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print('Saved sentiment-annotated CSV to', OUT_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28848edf",
   "metadata": {},
   "source": [
    "## 10) Aspect-based sentiment (phrase-level for topic keywords)\n",
    "Objectif : pour chaque article Forbes qui couvre un topic OMS, extraire phrases contenant topic keywords et évaluer le sentiment de ces phrases (approche simple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff3cfdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved aspect-level sentiment to outputs\\analysis_results\\aspect_sentiment_forbes.csv\n"
     ]
    }
   ],
   "source": [
    "# 10) Aspect-based sentiment\n",
    "import re\n",
    "try:\n",
    "    import nltk\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    from nltk import sent_tokenize\n",
    "except Exception:\n",
    "    def sent_tokenize(x):\n",
    "        return str(x).split('. ')\n",
    "\n",
    "aspect_records = []\n",
    "if os.path.exists(os.path.join(RESULTS_DIR, 'coverage_combined_forbes.csv')) and 'sent_pipe' in globals():\n",
    "    cov = pd.read_csv(os.path.join(RESULTS_DIR, 'coverage_combined_forbes.csv'))\n",
    "    for _, r in cov.iterrows():\n",
    "        gidx = int(r['global_index'])\n",
    "        art_text = df_clean.loc[gidx, 'texte_clean_bert']\n",
    "        for t in topic_topk.keys():\n",
    "            if r.get(f'covered_topic_{t}',0) == 1:\n",
    "                kws = topic_topk[t]\n",
    "                sents = [s for s in sent_tokenize(str(art_text)) if any(re.search(r'\\b'+re.escape(k)+r'\\b', s, flags=re.I) for k in kws)]\n",
    "                if not sents:\n",
    "                    sents = sent_tokenize(str(art_text))[:3]\n",
    "                scores = []\n",
    "                for s in sents:\n",
    "                    try:\n",
    "                        out = sent_pipe(s[:1000])[0]\n",
    "                        scores.append({'label': out['label'], 'score': float(out.get('score',0.0))})\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if scores:\n",
    "                    mapping = {'POSITIVE':1,'NEGATIVE':-1}\n",
    "                    vals = [mapping.get(x['label'].upper(),0)*x['score'] for x in scores]\n",
    "                    aspect_records.append({'global_index': gidx, 'topic': t, 'aspect_sentiment': float(np.mean(vals)), 'n_sentences': len(sents)})\n",
    "    aspect_df = pd.DataFrame(aspect_records)\n",
    "    aspect_df.to_csv(os.path.join(RESULTS_DIR, 'aspect_sentiment_forbes.csv'), index=False)\n",
    "    print('Saved aspect-level sentiment to', os.path.join(RESULTS_DIR, 'aspect_sentiment_forbes.csv'))\n",
    "else:\n",
    "    print('Either coverage file missing or sentiment pipeline not available; skip aspect-based sentiment')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e46a7",
   "metadata": {},
   "source": [
    "## 11) Framing (rule-based)\n",
    "Objectif : catégoriser le cadrage d'un article (economic / health / neutral / mixed) pour les articles Forbes couvrant un topic OMS via dictionnaires de mots-clés simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "271c8575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved framing to outputs\\analysis_results\\framing_forbes.csv\n"
     ]
    }
   ],
   "source": [
    "# 11) Framing rule-based\n",
    "ECON = set(['investissement','investir','marché','startup','business','venture','financement','investor','profit','entreprise'])\n",
    "HEALTH = set(['prévention','vaccin','vaccination','soins','sante','epidemie','mortalite','clinique','hopital','OMS','ministere'])\n",
    "framing = []\n",
    "if os.path.exists(os.path.join(RESULTS_DIR, 'coverage_combined_forbes.csv')):\n",
    "    cov = pd.read_csv(os.path.join(RESULTS_DIR, 'coverage_combined_forbes.csv'))\n",
    "    for _, r in cov.iterrows():\n",
    "        gidx = int(r['global_index'])\n",
    "        txt = set(str(df_clean.loc[gidx,'texte_clean_bert']).lower().split())\n",
    "        for t in topic_topk.keys():\n",
    "            if r.get(f'covered_topic_{t}',0) == 1:\n",
    "                econ = len(txt & ECON)\n",
    "                heal = len(txt & HEALTH)\n",
    "                if econ > heal and econ>=1:\n",
    "                    f = 'economic'\n",
    "                elif heal > econ and heal>=1:\n",
    "                    f = 'health'\n",
    "                elif econ==0 and heal==0:\n",
    "                    f = 'neutral'\n",
    "                else:\n",
    "                    f = 'mixed'\n",
    "                framing.append({'global_index': gidx, 'topic': t, 'framing': f})\n",
    "    framing_df = pd.DataFrame(framing)\n",
    "    framing_df.to_csv(os.path.join(RESULTS_DIR, 'framing_forbes.csv'), index=False)\n",
    "    print('Saved framing to', os.path.join(RESULTS_DIR, 'framing_forbes.csv'))\n",
    "else:\n",
    "    print('Coverage file missing; run coverage step first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b5bc3",
   "metadata": {},
   "source": [
    "## 12) Entity extraction (spaCy) et co-occurrence\n",
    "Objectif : extraire entités (ORG, PERSON, GPE) depuis les articles Forbes et produire un CSV utilisable pour construire un réseau d'acteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9d2eba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved entities to outputs\\analysis_results\\forbes_entities.csv\n"
     ]
    }
   ],
   "source": [
    "# 12) Entity extraction\n",
    "try:\n",
    "    import spacy\n",
    "    nlp_fr = spacy.load('fr_core_news_sm')\n",
    "    nlp_en = spacy.load('en_core_web_sm')\n",
    "except Exception as e:\n",
    "    print('spaCy models not installed or failed to load:', e)\n",
    "    nlp_fr = nlp_en = None\n",
    "\n",
    "ent_rows = []\n",
    "if nlp_fr is not None:\n",
    "    for idx, row in df_forbes.iterrows():\n",
    "        lang = row.get('lang','fr')\n",
    "        nlp = nlp_fr if str(lang).startswith('fr') else nlp_en\n",
    "        doc = nlp(str(row['texte_clean_bert']))\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in ('PER','ORG','GPE','LOC','MISC','PERSON'):\n",
    "                ent_rows.append({'global_index': int(idx), 'entity': ent.text, 'label': ent.label_})\n",
    "    ent_df = pd.DataFrame(ent_rows)\n",
    "    ent_df.to_csv(os.path.join(RESULTS_DIR, 'forbes_entities.csv'), index=False)\n",
    "    print('Saved entities to', os.path.join(RESULTS_DIR, 'forbes_entities.csv'))\n",
    "else:\n",
    "    print('spaCy not available — install the models to run NER')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b571da3",
   "metadata": {},
   "source": [
    "## 13) Appariement OMS → Forbes (nearest neighbors)\n",
    "Objectif : pour chaque document OMS, trouver les K articles Forbes les plus proches en similarité cosinus (embeddings). Permet une revue qualitative paire-à-paire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "001e388e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pairs to outputs\\analysis_results\\oms_to_forbes_pairs.csv\n"
     ]
    }
   ],
   "source": [
    "# 13) Nearest neighbors (brute force)\n",
    "if embeddings is not None and emb_oms.size and emb_forbes.size:\n",
    "    sims = cosine_similarity(emb_oms, emb_forbes)\n",
    "    top_k = 5\n",
    "    pairs = []\n",
    "    oms_global = df_clean.index[mask_oms].tolist()\n",
    "    for i in range(sims.shape[0]):\n",
    "        best_idx = np.argsort(sims[i])[-top_k:][::-1]\n",
    "        for rank, j in enumerate(best_idx):\n",
    "            pairs.append({'oms_index': int(oms_global[i]), 'forbes_index': int(df_clean.index[mask_forbes].tolist()[j]), 'rank': rank+1, 'similarity': float(sims[i, j])})\n",
    "    pairs_df = pd.DataFrame(pairs)\n",
    "    pairs_df.to_csv(os.path.join(RESULTS_DIR, 'oms_to_forbes_pairs.csv'), index=False)\n",
    "    print('Saved pairs to', os.path.join(RESULTS_DIR, 'oms_to_forbes_pairs.csv'))\n",
    "else:\n",
    "    print('Embeddings or subsets missing; cannot compute NN pairs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2490adaa",
   "metadata": {},
   "source": [
    "## 14) Tests statistiques simples\n",
    "Objectif : exemple de test pour vérifier si la couverture d'un topic OMS par Forbes est significativement différente (illustratif). Interpréter avec prudence sur petits corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c58c47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi2: 1.7949457711303183 p-value: 0.18032472947879363\n",
      "Saved simple stat test results to outputs\\analysis_results\\stat_tests.txt\n"
     ]
    }
   ],
   "source": [
    "# 14) Chi-square example (illustrative)\n",
    "import scipy.stats as stats\n",
    "cov_file = os.path.join(RESULTS_DIR, 'coverage_combined_forbes.csv')\n",
    "if os.path.exists(cov_file):\n",
    "    cov = pd.read_csv(cov_file)\n",
    "    topic = list(topic_topk.keys())[0]\n",
    "    covered = cov[f'covered_topic_{topic}'].sum()\n",
    "    not_covered = cov.shape[0] - covered\n",
    "    oms_count = len(noms_topic_assign.get(topic, [])) if 'noms_topic_assign' in globals() else 0\n",
    "    table = np.array([[covered, not_covered],[oms_count, max(1, len(df_oms)-oms_count)]])\n",
    "    chi2, p, dof, ex = stats.chi2_contingency(table)\n",
    "    print('Chi2:', chi2, 'p-value:', p)\n",
    "    with open(os.path.join(RESULTS_DIR, 'stat_tests.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write(f'Chi2 topic {topic}: {chi2}, p={p}\\n')\n",
    "    print('Saved simple stat test results to', os.path.join(RESULTS_DIR, 'stat_tests.txt'))\n",
    "else:\n",
    "    print('Coverage file missing; run coverage steps first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0734d84",
   "metadata": {},
   "source": [
    "## 15) Mots Cle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba078a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "# ------------- Mots-clés: TF-IDF top-n par document -------------\n",
    "# Paramètres\n",
    "TOP_K = 10\n",
    "\n",
    "\n",
    "# Construire TF-IDF global (unigrammes; on peut filtrer stopwords via vectorizer si besoin)\n",
    "tfidf = TfidfVectorizer(max_features=15000, ngram_range=(1,2))\n",
    "X_tfidf = tfidf.fit_transform(df_clean['texte_clean_tfidf'].fillna(''))\n",
    "feature_names = np.array(tfidf.get_feature_names_out())\n",
    "\n",
    "# Fonction pour obtenir top-k tokens d'une ligne TF-IDF\n",
    "def top_k_tfidf_row(row_vec, k=TOP_K):\n",
    "    if row_vec.nnz == 0:\n",
    "        return []\n",
    "    # row_vec is sparse row; get indices and data\n",
    "    idx = row_vec.indices\n",
    "    data = row_vec.data\n",
    "    # sort by value\n",
    "    order = np.argsort(data)[::-1]\n",
    "    top_idx = idx[order][:k]\n",
    "    return feature_names[top_idx].tolist()\n",
    "\n",
    "# Appliquer per-document\n",
    "keywords_list = []\n",
    "for i in range(X_tfidf.shape[0]):\n",
    "    kws = top_k_tfidf_row(X_tfidf[i], k=TOP_K)\n",
    "    keywords_list.append(\", \".join(kws))\n",
    "\n",
    "df_clean['keywords'] = keywords_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c5998",
   "metadata": {},
   "source": [
    "## 16) Export & rapport\n",
    "Les résultats (CSV/JSON) sont sauvegardés dans `<OUT_DIR>/analysis_results` :\n",
    "- `oms_topics.json` (topics OMS),\n",
    "- `lexical_coverage_forbes.csv`, `semantic_similarity_forbes.csv`, `coverage_combined_forbes.csv`,\n",
    "- `document_sentiment.csv`, `aspect_sentiment_forbes.csv`, `framing_forbes.csv`,\n",
    "- `forbes_entities.csv`, `oms_to_forbes_pairs.csv`, `stat_tests.txt`.\n",
    "\n",
    "Next steps recommandés : ajuster seuils, inspecter manuellement paires, calibrer les modèles de sentiment pour le français, et itérer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2274848",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv(\"../data/all_data_processed.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

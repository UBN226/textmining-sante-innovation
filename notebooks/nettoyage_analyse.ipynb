{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2bde0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "from transformers import pipeline\n",
    "from keybert import KeyBERT\n",
    "from sklearn.cluster import KMeans\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9c8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Télécharger les ressources NLTK\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"vader_lexicon\", quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c91baab",
   "metadata": {},
   "source": [
    "# 1) Fusion & inspection rapide\n",
    "\n",
    "**Objectif** : charger les deux CSV et créer un seul DataFrame maître avec une colonne source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b98e436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65, 5)\n",
      "['source', 'titre', 'date', 'lien', 'texte']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>titre</th>\n",
       "      <th>date</th>\n",
       "      <th>lien</th>\n",
       "      <th>texte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OMS</td>\n",
       "      <td>L’OMS publie des orientations pour faire face ...</td>\n",
       "      <td>3 novembre 2025</td>\n",
       "      <td>https://www.who.int/fr/news/item/03-11-2025-wh...</td>\n",
       "      <td>L’Organisation mondiale de la Santé (OMS) publ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OMS</td>\n",
       "      <td>L’OMS publie un guide mondial pour des société...</td>\n",
       "      <td>31 octobre 2025</td>\n",
       "      <td>https://www.who.int/fr/news/item/31-10-2025-wh...</td>\n",
       "      <td>À l’occasion de la Journée mondiale des villes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OMS</td>\n",
       "      <td>Lancement d’un cours en ligne sur l’utilisatio...</td>\n",
       "      <td>30 octobre 2025</td>\n",
       "      <td>https://www.who.int/fr/news/item/30-10-2025-la...</td>\n",
       "      <td>À l’occasion du Mois de la sensibilisation au ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OMS</td>\n",
       "      <td>L’OMS condamne le massacre de patients et de c...</td>\n",
       "      <td>29 octobre 2025</td>\n",
       "      <td>https://www.who.int/fr/news/item/29-10-2025-wh...</td>\n",
       "      <td>L’Organisation mondiale de la Santé (OMS) cond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OMS</td>\n",
       "      <td>Selon un nouveau rapport du Lancet Countdown, ...</td>\n",
       "      <td>29 octobre 2025</td>\n",
       "      <td>https://www.who.int/fr/news/item/29-10-2025-cl...</td>\n",
       "      <td>Alors qu’un nouveau rapport mondial publié auj...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source                                              titre             date  \\\n",
       "0    OMS  L’OMS publie des orientations pour faire face ...  3 novembre 2025   \n",
       "1    OMS  L’OMS publie un guide mondial pour des société...  31 octobre 2025   \n",
       "2    OMS  Lancement d’un cours en ligne sur l’utilisatio...  30 octobre 2025   \n",
       "3    OMS  L’OMS condamne le massacre de patients et de c...  29 octobre 2025   \n",
       "4    OMS  Selon un nouveau rapport du Lancet Countdown, ...  29 octobre 2025   \n",
       "\n",
       "                                                lien  \\\n",
       "0  https://www.who.int/fr/news/item/03-11-2025-wh...   \n",
       "1  https://www.who.int/fr/news/item/31-10-2025-wh...   \n",
       "2  https://www.who.int/fr/news/item/30-10-2025-la...   \n",
       "3  https://www.who.int/fr/news/item/29-10-2025-wh...   \n",
       "4  https://www.who.int/fr/news/item/29-10-2025-cl...   \n",
       "\n",
       "                                               texte  \n",
       "0  L’Organisation mondiale de la Santé (OMS) publ...  \n",
       "1  À l’occasion de la Journée mondiale des villes...  \n",
       "2  À l’occasion du Mois de la sensibilisation au ...  \n",
       "3  L’Organisation mondiale de la Santé (OMS) cond...  \n",
       "4  Alors qu’un nouveau rapport mondial publié auj...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oms = pd.read_csv(\"../outputs/articles_oms.csv\")      \n",
    "forbes = pd.read_csv(\"../outputs/forbes_articles.csv\")\n",
    "\n",
    "oms['source'] = 'OMS'\n",
    "forbes['source'] = 'Forbes'\n",
    "df = pd.concat([oms, forbes], ignore_index=True)\n",
    "\n",
    "# aperçu\n",
    "print(df.shape)\n",
    "print(df.columns.tolist())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01e5c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/all_articles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b5ee19",
   "metadata": {},
   "source": [
    "# 2) Nettoyage de base (HTML, espaces, encodage, dates)\n",
    "\n",
    "**Objectif** : normaliser le texte brut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7930326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Nettoyage léger (pour BERT) et prétraitement pour TF-IDF ===\n",
    "\n",
    "\n",
    "nlp_fr = spacy.load(\"fr_core_news_sm\", disable=[\"parser\",\"ner\"])\n",
    "nlp_en = spacy.load(\"en_core_web_sm\", disable=[\"parser\",\"ner\"])\n",
    "\n",
    "def clean_html_only(text):\n",
    "    \"\"\"Nettoyage léger : retire script/style/URLs, conserve phrases intactes.\"\"\"\n",
    "    soup = BeautifulSoup(str(text), \"html.parser\")\n",
    "    for s in soup([\"script\",\"style\"]):\n",
    "        s.decompose()\n",
    "    out = soup.get_text(separator=\" \")\n",
    "    out = re.sub(r\"https?://\\S+|www\\.\\S+|\\S+@\\S+\", \" \", out)\n",
    "    out = out.replace(\"’\", \"'\").replace(\"‘\",\"'\")\n",
    "    out = re.sub(r\"\\s+\", \" \", out).strip()\n",
    "    txt = re.sub(r'javascript.*disabled.*', ' ', text, flags=re.I)\n",
    "    txt = re.sub(r'cookie.*', ' ', text, flags=re.I)\n",
    "    return out\n",
    "\n",
    "def preprocess_for_tfidf(text, lang_hint='fr', min_tok=2):\n",
    "    \"\"\"Nettoyage plus agressif pour TF-IDF / LDA : lowercase, remove stopwords, lemmatisation.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text or \"\")\n",
    "    text = clean_html_only(text)\n",
    "    text = text.lower()\n",
    "    # keep letters & accents & apostrophes and spaces\n",
    "    text = re.sub(r\"[^a-z0-9àâäçéèêëîïôöùûüÿœæ'\\s-]\", \" \", text)\n",
    "    doc = (nlp_fr if str(lang_hint).startswith(\"fr\") else nlp_en)(text)\n",
    "    toks = []\n",
    "    for t in doc:\n",
    "        if t.is_stop or t.is_punct or t.is_space or t.like_num:\n",
    "            continue\n",
    "        lemma = t.lemma_.lower().strip()\n",
    "        if len(lemma) < min_tok:\n",
    "            continue\n",
    "        toks.append(lemma)\n",
    "    return \" \".join(toks)\n",
    "\n",
    "# detection de langue\n",
    "\n",
    "def safe_detect(s):\n",
    "    try: return detect(s)\n",
    "    except: return 'unknown'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e6ef5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texte</th>\n",
       "      <th>texte_clean_bert</th>\n",
       "      <th>texte_clean_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L’Organisation mondiale de la Santé (OMS) publ...</td>\n",
       "      <td>L'Organisation mondiale de la Santé (OMS) publ...</td>\n",
       "      <td>organisation mondial santé oms publier aujourd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>À l’occasion de la Journée mondiale des villes...</td>\n",
       "      <td>À l'occasion de la Journée mondiale des villes...</td>\n",
       "      <td>occasion journée mondial ville organisation mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>À l’occasion du Mois de la sensibilisation au ...</td>\n",
       "      <td>À l'occasion du Mois de la sensibilisation au ...</td>\n",
       "      <td>occasion mois sensibilisation cancer sein cent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texte  \\\n",
       "0  L’Organisation mondiale de la Santé (OMS) publ...   \n",
       "1  À l’occasion de la Journée mondiale des villes...   \n",
       "2  À l’occasion du Mois de la sensibilisation au ...   \n",
       "\n",
       "                                    texte_clean_bert  \\\n",
       "0  L'Organisation mondiale de la Santé (OMS) publ...   \n",
       "1  À l'occasion de la Journée mondiale des villes...   \n",
       "2  À l'occasion du Mois de la sensibilisation au ...   \n",
       "\n",
       "                                   texte_clean_tfidf  \n",
       "0  organisation mondial santé oms publier aujourd...  \n",
       "1  occasion journée mondial ville organisation mo...  \n",
       "2  occasion mois sensibilisation cancer sein cent...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df.copy()\n",
    "\n",
    "\n",
    "df_clean['texte_clean_bert'] = df['texte'].astype(str).apply(clean_html_only)\n",
    "df_clean['lang'] = df_clean['texte_clean_bert'].apply(lambda s: safe_detect(s) if s.strip() else 'unknown')\n",
    "df_clean['texte_clean_tfidf'] = df_clean.apply(lambda r: preprocess_for_tfidf(r['texte'], lang_hint=r['lang']), axis=1)\n",
    "\n",
    "# vérifier quelques exemples\n",
    "df_clean[['texte','texte_clean_bert','texte_clean_tfidf']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07fd774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('de', 2195),\n",
       " ('et', 1192),\n",
       " ('la', 1074),\n",
       " ('des', 884),\n",
       " ('les', 821),\n",
       " ('à', 777),\n",
       " ('le', 578),\n",
       " ('en', 573),\n",
       " ('du', 431),\n",
       " ('pour', 342),\n",
       " ('un', 330),\n",
       " ('dans', 320),\n",
       " ('une', 246),\n",
       " ('sur', 229),\n",
       " ('plus', 228),\n",
       " ('que', 213),\n",
       " ('est', 212),\n",
       " ('aux', 210),\n",
       " ('qui', 193),\n",
       " ('santé', 193),\n",
       " ('au', 184),\n",
       " ('a', 180),\n",
       " ('par', 157),\n",
       " ('«', 149),\n",
       " (':', 139),\n",
       " ('avec', 126),\n",
       " ('sont', 117),\n",
       " ('pays', 108),\n",
       " ('ou', 101),\n",
       " ('Le', 91)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = ' '.join(df_clean['texte_clean_bert'].astype(str).tolist()).split()\n",
    "after = ' '.join(df_clean['texte_clean_tfidf'].astype(str).tolist()).split()\n",
    "Counter(before).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "088677de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('santé', 252),\n",
       " ('pays', 145),\n",
       " ('om', 130),\n",
       " ('mondial', 99),\n",
       " ('numérique', 82),\n",
       " ('africain', 82),\n",
       " ('être', 79),\n",
       " ('afrique', 78),\n",
       " ('développement', 69),\n",
       " ('national', 67),\n",
       " ('service', 66),\n",
       " ('international', 66),\n",
       " ('secteur', 63),\n",
       " ('système', 61),\n",
       " ('nouveau', 58),\n",
       " ('monde', 58),\n",
       " ('stratégique', 55),\n",
       " ('renforcer', 55),\n",
       " ('projet', 54),\n",
       " ('local', 53),\n",
       " (\"aujourd'hui\", 52),\n",
       " ('an', 52),\n",
       " ('million', 49),\n",
       " ('personne', 48),\n",
       " ('soin', 47),\n",
       " ('continent', 47),\n",
       " ('femme', 45),\n",
       " ('entreprise', 45),\n",
       " ('investissement', 44),\n",
       " ('faire', 43)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(after).most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc68879",
   "metadata": {},
   "source": [
    "# 3) Déduplication & filtrage\n",
    "\n",
    "**Objectif** : enlever doublons exacts et presque-identiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea62491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doublons exacts sur le texte\n",
    "df_clean = df_clean.drop_duplicates(subset=['texte']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5c6ea5",
   "metadata": {},
   "source": [
    "# 5) Prétraitement linguistique (tokenize / stopwords / lemmatisation)\n",
    "\n",
    "**Objectif** : préparer texte pour modèles classiques (TF-IDF, LDA) et pour embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91573272",
   "metadata": {},
   "source": [
    "# 6) Représentation numérique\n",
    "\n",
    "A) **TF-IDF** (rapide, interprétable) — pour classification, clustering léger, recherche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95f8c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=15000, ngram_range=(1,2))\n",
    "X_tfidf = tfidf.fit_transform(df_clean['texte_clean_tfidf'].fillna(''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d53041",
   "metadata": {},
   "source": [
    "B) **Embeddings** (BERT / SentenceTransformers) — mieux pour clustering sémantique, topics modernes, similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "813e9ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:22<00:00, 11.19s/it]\n"
     ]
    }
   ],
   "source": [
    "# pip install sentence-transformers\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "embeddings = model.encode(df_clean['texte_clean_bert'].tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ee777f",
   "metadata": {},
   "source": [
    "# 7) Topic Modeling\n",
    "\n",
    "* Si TF-IDF -> **NMF** (souvent meilleur) ou **LDA** (gensim).\n",
    "* Si embeddings -> **BERTopic** ou clustering + keywords extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25135e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.011*\"om\" + 0.009*\"santé\" + 0.008*\"produit\" + 0.007*\"être\" + 0.007*\"public\" + 0.006*\"trachome\" + 0.005*\"autorité\" + 0.005*\"fidji\" + 0.004*\"sanitaire\" + 0.004*\"cancer\"\n",
      "1 0.007*\"cookie\" + 0.006*\"site\" + 0.006*\"agricole\" + 0.006*\"être\" + 0.005*\"risque\" + 0.005*\"afrique\" + 0.004*\"terminal\" + 0.004*\"hpp\" + 0.004*\"femme\" + 0.004*\"pays\"\n",
      "2 0.009*\"africain\" + 0.007*\"local\" + 0.007*\"afrique\" + 0.006*\"million\" + 0.006*\"tabac\" + 0.006*\"mondial\" + 0.006*\"numérique\" + 0.005*\"banque\" + 0.005*\"financement\" + 0.005*\"sfi\"\n",
      "3 0.009*\"développement\" + 0.009*\"gabonais\" + 0.009*\"pays\" + 0.006*\"gabon\" + 0.006*\"tourisme\" + 0.005*\"projet\" + 0.005*\"afrique\" + 0.005*\"secteur\" + 0.005*\"national\" + 0.004*\"durable\"\n",
      "4 0.032*\"santé\" + 0.015*\"om\" + 0.012*\"pays\" + 0.008*\"mondial\" + 0.007*\"système\" + 0.006*\"personne\" + 0.006*\"soin\" + 0.005*\"maladie\" + 0.004*\"numérique\" + 0.004*\"cours\"\n",
      "5 0.012*\"numérique\" + 0.007*\"africain\" + 0.007*\"cap-vert\" + 0.007*\"secteur\" + 0.006*\"dangote\" + 0.006*\"pays\" + 0.006*\"développement\" + 0.005*\"stratégique\" + 0.005*\"international\" + 0.005*\"renforcer\"\n",
      "6 0.005*\"entreprise\" + 0.005*\"gabon\" + 0.005*\"maldive\" + 0.005*\"continent\" + 0.005*\"enfant\" + 0.005*\"voiture\" + 0.005*\"digital\" + 0.004*\"local\" + 0.004*\"être\" + 0.004*\"partenaire\"\n",
      "7 0.003*\"continent\" + 0.003*\"africain\" + 0.003*\"an\" + 0.003*\"petit\" + 0.003*\"meubler\" + 0.003*\"sérieux\" + 0.003*\"concept\" + 0.002*\"financier\" + 0.002*\"jeunesse\" + 0.002*\"secteur\"\n"
     ]
    }
   ],
   "source": [
    "texts = [t.split() for t in df_clean['texte_clean_tfidf']]\n",
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(t) for t in texts]\n",
    "lda = gensim.models.LdaModel(corpus, id2word=dictionary, num_topics=8, passes=10, random_state=42)\n",
    "\n",
    "for i,topic in lda.print_topics(-1):\n",
    "    print(i, topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc9743a",
   "metadata": {},
   "source": [
    "# 8) Clustering d’articles\n",
    "\n",
    "KMeans sur TF-IDF ou HDBSCAN sur embeddings (si grand jeu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1707a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=8).fit(X_tfidf)\n",
    "df_clean['cluster_tfidf'] = kmeans.predict(X_tfidf)\n",
    "\n",
    "# k = 8\n",
    "# km = KMeans(n_clusters=k, random_state=42)\n",
    "# clusters = km.fit_predict(X_tfidf)\n",
    "# df['cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7904b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN on embeddings\n",
    "import hdbscan\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5).fit(embeddings)\n",
    "df_clean['cluster_embed'] = clusterer.labels_\n",
    "\n",
    "# clusterer = hdbscan.HDBSCAN(min_cluster_size=5)\n",
    "# df['cluster_emb'] = clusterer.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d825a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da166b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'umap' has no attribute 'UMAP'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mumap\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m reducer = \u001b[43mumap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mUMAP\u001b[49m(n_neighbors=\u001b[32m15\u001b[39m, min_dist=\u001b[32m0.1\u001b[39m)\n\u001b[32m      4\u001b[39m umap_tf = reducer.fit_transform(X_tfidf)\n\u001b[32m      5\u001b[39m umap_emb = reducer.fit_transform(embeddings)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'umap' has no attribute 'UMAP'"
     ]
    }
   ],
   "source": [
    "import umap.umap_ as umap\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)\n",
    "umap_tf = reducer.fit_transform(X_tfidf)\n",
    "umap_emb = reducer.fit_transform(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ae2db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1ed0eec",
   "metadata": {},
   "source": [
    "# 9) Extraction de mots-clé / résumés / phrases clés\n",
    "\n",
    "* Keywords : RAKE / YAKE / KeyBERT.\n",
    "* Résumé : transformers (Bart, T5) — extractive vs abstractive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "572d9213",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "kw = KeyBERT(model='all-mpnet-base-v2')\n",
    "df_clean['keywords'] = df_clean['texte_clean_bert'].apply(lambda t: kw.extract_keywords(t, top_n=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c90ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "df['summary'] = df['text'].apply(lambda t: summarizer(t, max_length=60, min_length=20)[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601dcf45",
   "metadata": {},
   "source": [
    "# 10) Sentiment & ton / polarité\n",
    "\n",
    "* Pour l’anglais: modèles HuggingFace (sst2, cardiffnlp).\n",
    "* Pour le français: CamemBERT fine-tuned (ou modèle multilingual)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dfea017d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "d:\\Mes_dossiers\\IDSI\\M2\\Course\\Text_Mining\\textmining-sante-innovation\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ulric\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sent = pipeline('sentiment-analysis')\n",
    "df_clean['sentiment'] = df_clean['texte_clean_bert'].apply(lambda t: sent(t[:512])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae11d26",
   "metadata": {},
   "source": [
    "# 11) NER (entités nommées)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8d8357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text, lang='fr'):\n",
    "    nlp = nlp_fr if lang=='fr' else nlp_en\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "df['entities'] = df.apply(lambda r: extract_entities(r['text'], r['lang']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ed9d7d",
   "metadata": {},
   "source": [
    "# 12) Analyse comparative *OMS vs Forbes*\n",
    "\n",
    "**Objectifs d’analyse** :\n",
    "\n",
    "* Distribution des topics par source.\n",
    "* Mots-clés distinctifs (chi2 ou log-likelihood) par source.\n",
    "* Sentiment moyen par source.\n",
    "* Entités les plus citées par source.\n",
    "\n",
    "Exemple : mots discriminants via chi2 (scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16bfd4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['santé',\n",
       " 'om',\n",
       " 'soin',\n",
       " 'système',\n",
       " 'sanitaire',\n",
       " 'maladie',\n",
       " 'personne',\n",
       " 'mental',\n",
       " 'système santé',\n",
       " 'tabac',\n",
       " 'pacifique',\n",
       " 'santé mental',\n",
       " 'cours',\n",
       " 'cancer',\n",
       " 'résistance',\n",
       " 'neurologique',\n",
       " 'enfant',\n",
       " 'apprentissage',\n",
       " 'vaccin',\n",
       " 'trachome',\n",
       " 'el fasher',\n",
       " 'fasher',\n",
       " 'infection',\n",
       " 'rapport',\n",
       " 'santé numérique',\n",
       " 'fidji',\n",
       " 'surveillance',\n",
       " 'hpp',\n",
       " 'mondial',\n",
       " 'health']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "y = (df_clean['source']=='OMS').astype(int)\n",
    "chi2scores, p = chi2(X_tfidf, y)\n",
    "top_n = 30\n",
    "terms = tfidf.get_feature_names_out()\n",
    "top_terms = [terms[i] for i in chi2scores.argsort()[-top_n:][::-1]]\n",
    "top_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a67c9",
   "metadata": {},
   "source": [
    "# 13) Visualisation & reporting\n",
    "\n",
    "* Wordclouds par source.\n",
    "* t-SNE / UMAP des embeddings pour voir clusters.\n",
    "* Barplots des topics, entités, sentiment.\n",
    "\n",
    "Exemple UMAP + scatter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8ded09d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'umap' has no attribute 'UMAP'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mumap\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m reducer = \u001b[43mumap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mUMAP\u001b[49m(n_neighbors=\u001b[32m15\u001b[39m, min_dist=\u001b[32m0.1\u001b[39m, metric=\u001b[33m'\u001b[39m\u001b[33mcosine\u001b[39m\u001b[33m'\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      3\u001b[39m emb_2d = reducer.fit_transform(embeddings)\n\u001b[32m      4\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mx\u001b[39m\u001b[33m'\u001b[39m], df[\u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m] = emb_2d[:,\u001b[32m0\u001b[39m], emb_2d[:,\u001b[32m1\u001b[39m]\n",
      "\u001b[31mAttributeError\u001b[39m: module 'umap' has no attribute 'UMAP'"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)\n",
    "emb_2d = reducer.fit_transform(embeddings)\n",
    "df['x'], df['y'] = emb_2d[:,0], emb_2d[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0844b",
   "metadata": {},
   "source": [
    "# 14) Export des résultats\n",
    "\n",
    "Sauvegarder DataFrame enrichi :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7dca8617",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv(\"../data/all_articles_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db069bf7",
   "metadata": {},
   "source": [
    "## Récapitulatif rapide (ordre d’exécution conseillé)\n",
    "\n",
    "1. Fusion + inspection\n",
    "2. Nettoyage HTML + normalisation\n",
    "3. Déduplication + détection langue\n",
    "4. Prétraitement spaCy (lemmatisation, stopwords)\n",
    "5. Vecteurs (TF-IDF et/ou embeddings)\n",
    "6. Topic modelling + clustering\n",
    "7. NER + keywords + résumé + sentiment\n",
    "8. Analyses comparatives OMS vs Forbes\n",
    "9. Visualisations & export\n",
    "10. QA manuelle et itérations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

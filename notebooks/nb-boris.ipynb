{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a3d7384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMS → 13 liens\n",
      "Forbes → 1 liens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMS - Extraction: 100%|██████████| 13/13 [00:25<00:00,  1.98s/it]\n",
      "Forbes - Extraction: 100%|██████████| 1/1 [00:02<00:00,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exporté: outputs\\raw_articles_oms_forbes.csv (9 articles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Phase 1 — Scraping OMS & Forbes Afrique\n",
    "Objectif : collecter des articles récents sur la santé (OMS) et l'innovation (Forbes Afrique).\n",
    "\n",
    "Sortie principale :\n",
    "  outputs/raw_articles_oms_forbes.csv  (colonnes: source,title,date,url,text)\n",
    "\n",
    "Usage :\n",
    "  pip install -r requirements.txt\n",
    "  python phase1_scrape.py\n",
    "\"\"\"\n",
    "\n",
    "import os, re, time, random\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from dateutil import parser as dateparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------- Config de base ----------\n",
    "HEADERS = {\"User-Agent\": \"TextMiningStudentProject/1.0 (+https://example.org)\"}\n",
    "MAX_ARTICLES_PER_SOURCE = 20          # <-- augmente/diminue si nécessaire\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "OUTPUT_CSV = os.path.join(OUTPUT_DIR, \"raw_articles_oms_forbes.csv\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --------- Utilitaires HTTP ----------\n",
    "\n",
    "def get_html(url, headers=HEADERS, timeout=30, retries=2, backoff=1.3):\n",
    "    \"\"\"Télécharge le HTML avec quelques tentatives et un backoff exponentiel simple.\"\"\"\n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            r = requests.get(url, headers=headers, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r.text\n",
    "        except Exception as e:\n",
    "            if attempt < retries:\n",
    "                # on patiente un peu avant de réessayer (respect du site)\n",
    "                time.sleep((backoff ** attempt) + random.random() * 0.3)\n",
    "            else:\n",
    "                print(f\"[ERREUR] {url} -> {e}\")\n",
    "                return None\n",
    "\n",
    "def absolutize(base_url, href):\n",
    "    \"\"\"Transforme un lien relatif en lien absolu.\"\"\"\n",
    "    return urljoin(base_url, href) if href else None\n",
    "\n",
    "def is_same_domain(url, base_domain):\n",
    "    \"\"\"Vérifie que l'URL appartient au domaine attendu (sécurité et propreté).\"\"\"\n",
    "    try:\n",
    "        return base_domain in urlparse(url).netloc.lower()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    \"\"\"Nettoyage léger : trim et espaces multiples.\"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
    "\n",
    "# --------- OMS : liste + parsing ----------\n",
    "\n",
    "def list_oms_articles(max_links=MAX_ARTICLES_PER_SOURCE):\n",
    "    \"\"\"Récupère des liens plausibles d'articles OMS (FR) depuis la page News.\"\"\"\n",
    "    base = \"https://www.who.int\"\n",
    "    listing = \"https://www.who.int/fr/news\"\n",
    "    html = get_html(listing)\n",
    "    if not html:\n",
    "        return []\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a.get(\"href\")\n",
    "        # heuristique: liens FR + 'news'\n",
    "        if href and (\"/fr/\" in href) and (\"/news\" in href):\n",
    "            url_abs = absolutize(base, href)\n",
    "            if url_abs and url_abs not in links:\n",
    "                links.append(url_abs)\n",
    "    return links[:max_links]\n",
    "\n",
    "def parse_oms_article(url):\n",
    "    \"\"\"Extrait (title, date, text) d'un article OMS.\"\"\"\n",
    "    html = get_html(url)\n",
    "    if not html:\n",
    "        return None\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # titre\n",
    "    title_tag = soup.find([\"h1\", \"title\"])\n",
    "    title = clean_text(title_tag.get_text(\" \", strip=True)) if title_tag else \"\"\n",
    "\n",
    "    # date (time[datetime] puis meta article:published_time)\n",
    "    date_iso = None\n",
    "    t = soup.find(\"time\")\n",
    "    if t and t.get(\"datetime\"):\n",
    "        date_iso = t.get(\"datetime\")\n",
    "    if not date_iso:\n",
    "        meta = soup.find(\"meta\", {\"property\": \"article:published_time\"})\n",
    "        if meta and meta.get(\"content\"):\n",
    "            date_iso = meta.get(\"content\")\n",
    "    try:\n",
    "        date_str = dateparser.parse(date_iso).isoformat() if date_iso else None\n",
    "    except Exception:\n",
    "        date_str = None\n",
    "\n",
    "    # texte (concat des <p> non trop courts)\n",
    "    paragraphs = [p.get_text(\" \", strip=True) for p in soup.find_all(\"p\")]\n",
    "    paragraphs = [clean_text(p) for p in paragraphs if len(p.split()) >= 5]\n",
    "    text = \"\\n\".join(paragraphs)\n",
    "\n",
    "    return {\"source\": \"OMS\", \"url\": url, \"title\": title, \"date\": date_str, \"text\": text}\n",
    "\n",
    "# --------- Forbes Afrique : liste + parsing ----------\n",
    "\n",
    "def list_forbes_articles(max_links=MAX_ARTICLES_PER_SOURCE):\n",
    "    \"\"\"Récupère des liens plausibles d'articles Forbes Afrique depuis la homepage.\"\"\"\n",
    "    start = \"https://www.forbesafrique.com/\"\n",
    "    domain = \"forbesafrique.com\"\n",
    "    html = get_html(start)\n",
    "    if not html:\n",
    "        return []\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a.get(\"href\")\n",
    "        url_abs = absolutize(start, href)\n",
    "        if url_abs and is_same_domain(url_abs, domain):\n",
    "            # Heuristiques d'articles : année/mois dans l'URL, 'article' dans le slug...\n",
    "            if re.search(r\"/\\d{4}/\\d{2}/\", url_abs) or (\"article\" in url_abs.lower()) or (\"/202\" in url_abs):\n",
    "                if url_abs not in links:\n",
    "                    links.append(url_abs)\n",
    "    return links[:max_links]\n",
    "\n",
    "def parse_forbes_article(url):\n",
    "    \"\"\"Extrait (title, date, text) d'un article Forbes Afrique.\"\"\"\n",
    "    html = get_html(url)\n",
    "    if not html:\n",
    "        return None\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    title_tag = soup.find(\"h1\") or soup.find(\"title\")\n",
    "    title = clean_text(title_tag.get_text(\" \", strip=True)) if title_tag else \"\"\n",
    "\n",
    "    # date via <time> ou meta\n",
    "    date_iso = None\n",
    "    t = soup.find(\"time\")\n",
    "    if t and (t.get(\"datetime\") or t.get_text(strip=True)):\n",
    "        date_iso = t.get(\"datetime\") or t.get_text(strip=True)\n",
    "    if not date_iso:\n",
    "        meta = soup.find(\"meta\", {\"property\": \"article:published_time\"})\n",
    "        if meta and meta.get(\"content\"):\n",
    "            date_iso = meta.get(\"content\")\n",
    "    try:\n",
    "        date_str = dateparser.parse(date_iso).isoformat() if date_iso else None\n",
    "    except Exception:\n",
    "        date_str = None\n",
    "\n",
    "    paragraphs = [p.get_text(\" \", strip=True) for p in soup.find_all(\"p\")]\n",
    "    paragraphs = [clean_text(p) for p in paragraphs if len(p.split()) >= 5]\n",
    "    text = \"\\n\".join(paragraphs)\n",
    "\n",
    "    return {\"source\": \"Forbes\", \"url\": url, \"title\": title, \"date\": date_str, \"text\": text}\n",
    "\n",
    "# --------- Orchestration ----------\n",
    "\n",
    "def main():\n",
    "    rows = []\n",
    "\n",
    "    # 1) collecter les liens\n",
    "    oms_links = list_oms_articles(MAX_ARTICLES_PER_SOURCE)\n",
    "    print(f\"OMS → {len(oms_links)} liens\")\n",
    "\n",
    "    forbes_links = list_forbes_articles(MAX_ARTICLES_PER_SOURCE)\n",
    "    print(f\"Forbes → {len(forbes_links)} liens\")\n",
    "\n",
    "    # 2) parser OMS\n",
    "    for u in tqdm(oms_links, desc=\"OMS - Extraction\"):\n",
    "        row = parse_oms_article(u)\n",
    "        time.sleep(0.8 + random.random() * 0.4)  # respect du site\n",
    "        if row and row.get(\"text\"):\n",
    "            rows.append(row)\n",
    "\n",
    "    # 3) parser Forbes\n",
    "    for u in tqdm(forbes_links, desc=\"Forbes - Extraction\"):\n",
    "        row = parse_forbes_article(u)\n",
    "        time.sleep(0.8 + random.random() * 0.4)\n",
    "        if row and row.get(\"text\"):\n",
    "            rows.append(row)\n",
    "\n",
    "    # 4) consolidation\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        print(\"⚠️ Aucun article valide n'a été extrait.\")\n",
    "        return\n",
    "\n",
    "    # nettoyage léger + filtrage\n",
    "    df[\"title\"] = df[\"title\"].fillna(\"\").map(clean_text)\n",
    "    df[\"text\"] = df[\"text\"].fillna(\"\").map(clean_text)\n",
    "    df = df[df[\"text\"].str.len() > 100]\n",
    "    df = df.drop_duplicates(subset=[\"url\"]).reset_index(drop=True)\n",
    "\n",
    "    # 5) export\n",
    "    df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "    print(f\"✅ Exporté: {OUTPUT_CSV} ({len(df)} articles)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "121ff8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMS → 13 liens\n",
      "Forbes → 1 liens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMS - Extraction: 100%|██████████| 13/13 [00:24<00:00,  1.89s/it]\n",
      "Forbes - Extraction: 100%|██████████| 1/1 [00:02<00:00,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exporté: outputs\\raw_articles_oms_forbes.csv (9 articles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Phase 1 — Scraping OMS & Forbes Afrique\n",
    "Objectif : collecter des articles récents sur la santé (OMS) et l'innovation (Forbes Afrique).\n",
    "\n",
    "Sortie principale :\n",
    "  outputs/raw_articles_oms_forbes.csv  (colonnes: source,title,date,url,text)\n",
    "\n",
    "Usage :\n",
    "  pip install -r requirements.txt\n",
    "  python phase1_scrape.py\n",
    "\"\"\"\n",
    "\n",
    "import os, re, time, random\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from dateutil import parser as dateparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------- Config de base ----------\n",
    "HEADERS = {\"User-Agent\": \"TextMiningStudentProject/1.0 (+https://example.org)\"}\n",
    "MAX_ARTICLES_PER_SOURCE = 20          # <-- augmente/diminue si nécessaire\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "OUTPUT_CSV = os.path.join(OUTPUT_DIR, \"raw_articles_oms_forbes.csv\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --------- Utilitaires HTTP ----------\n",
    "\n",
    "def get_html(url, headers=HEADERS, timeout=30, retries=2, backoff=1.3):\n",
    "    \"\"\"Télécharge le HTML avec quelques tentatives et un backoff exponentiel simple.\"\"\"\n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            r = requests.get(url, headers=headers, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r.text\n",
    "        except Exception as e:\n",
    "            if attempt < retries:\n",
    "                # on patiente un peu avant de réessayer (respect du site)\n",
    "                time.sleep((backoff ** attempt) + random.random() * 0.3)\n",
    "            else:\n",
    "                print(f\"[ERREUR] {url} -> {e}\")\n",
    "                return None\n",
    "\n",
    "def absolutize(base_url, href):\n",
    "    \"\"\"Transforme un lien relatif en lien absolu.\"\"\"\n",
    "    return urljoin(base_url, href) if href else None\n",
    "\n",
    "def is_same_domain(url, base_domain):\n",
    "    \"\"\"Vérifie que l'URL appartient au domaine attendu (sécurité et propreté).\"\"\"\n",
    "    try:\n",
    "        return base_domain in urlparse(url).netloc.lower()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    \"\"\"Nettoyage léger : trim et espaces multiples.\"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
    "\n",
    "# --------- OMS : liste + parsing ----------\n",
    "\n",
    "def list_oms_articles(max_links=MAX_ARTICLES_PER_SOURCE):\n",
    "    \"\"\"Récupère des liens plausibles d'articles OMS (FR) depuis la page News.\"\"\"\n",
    "    base = \"https://www.who.int\"\n",
    "    listing = \"https://www.who.int/fr/news\"\n",
    "    html = get_html(listing)\n",
    "    if not html:\n",
    "        return []\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a.get(\"href\")\n",
    "        # heuristique: liens FR + 'news'\n",
    "        if href and (\"/fr/\" in href) and (\"/news\" in href):\n",
    "            url_abs = absolutize(base, href)\n",
    "            if url_abs and url_abs not in links:\n",
    "                links.append(url_abs)\n",
    "    return links[:max_links]\n",
    "\n",
    "def parse_oms_article(url):\n",
    "    \"\"\"Extrait (title, date, text) d'un article OMS.\"\"\"\n",
    "    html = get_html(url)\n",
    "    if not html:\n",
    "        return None\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # titre\n",
    "    title_tag = soup.find([\"h1\", \"title\"])\n",
    "    title = clean_text(title_tag.get_text(\" \", strip=True)) if title_tag else \"\"\n",
    "\n",
    "    # date (time[datetime] puis meta article:published_time)\n",
    "    date_iso = None\n",
    "    t = soup.find(\"time\")\n",
    "    if t and t.get(\"datetime\"):\n",
    "        date_iso = t.get(\"datetime\")\n",
    "    if not date_iso:\n",
    "        meta = soup.find(\"meta\", {\"property\": \"article:published_time\"})\n",
    "        if meta and meta.get(\"content\"):\n",
    "            date_iso = meta.get(\"content\")\n",
    "    try:\n",
    "        date_str = dateparser.parse(date_iso).isoformat() if date_iso else None\n",
    "    except Exception:\n",
    "        date_str = None\n",
    "\n",
    "    # texte (concat des <p> non trop courts)\n",
    "    paragraphs = [p.get_text(\" \", strip=True) for p in soup.find_all(\"p\")]\n",
    "    paragraphs = [clean_text(p) for p in paragraphs if len(p.split()) >= 5]\n",
    "    text = \"\\n\".join(paragraphs)\n",
    "\n",
    "    return {\"source\": \"OMS\", \"url\": url, \"title\": title, \"date\": date_str, \"text\": text}\n",
    "\n",
    "# --------- Forbes Afrique : liste + parsing ----------\n",
    "\n",
    "def list_forbes_articles(max_links=MAX_ARTICLES_PER_SOURCE):\n",
    "    \"\"\"Récupère des liens plausibles d'articles Forbes Afrique depuis la homepage.\"\"\"\n",
    "    start = \"https://www.forbesafrique.com/\"\n",
    "    domain = \"forbesafrique.com\"\n",
    "    html = get_html(start)\n",
    "    if not html:\n",
    "        return []\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a.get(\"href\")\n",
    "        url_abs = absolutize(start, href)\n",
    "        if url_abs and is_same_domain(url_abs, domain):\n",
    "            # Heuristiques d'articles : année/mois dans l'URL, 'article' dans le slug...\n",
    "            if re.search(r\"/\\d{4}/\\d{2}/\", url_abs) or (\"article\" in url_abs.lower()) or (\"/202\" in url_abs):\n",
    "                if url_abs not in links:\n",
    "                    links.append(url_abs)\n",
    "    return links[:max_links]\n",
    "\n",
    "def parse_forbes_article(url):\n",
    "    \"\"\"Extrait (title, date, text) d'un article Forbes Afrique.\"\"\"\n",
    "    html = get_html(url)\n",
    "    if not html:\n",
    "        return None\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    title_tag = soup.find(\"h1\") or soup.find(\"title\")\n",
    "    title = clean_text(title_tag.get_text(\" \", strip=True)) if title_tag else \"\"\n",
    "\n",
    "    # date via <time> ou meta\n",
    "    date_iso = None\n",
    "    t = soup.find(\"time\")\n",
    "    if t and (t.get(\"datetime\") or t.get_text(strip=True)):\n",
    "        date_iso = t.get(\"datetime\") or t.get_text(strip=True)\n",
    "    if not date_iso:\n",
    "        meta = soup.find(\"meta\", {\"property\": \"article:published_time\"})\n",
    "        if meta and meta.get(\"content\"):\n",
    "            date_iso = meta.get(\"content\")\n",
    "    try:\n",
    "        date_str = dateparser.parse(date_iso).isoformat() if date_iso else None\n",
    "    except Exception:\n",
    "        date_str = None\n",
    "\n",
    "    paragraphs = [p.get_text(\" \", strip=True) for p in soup.find_all(\"p\")]\n",
    "    paragraphs = [clean_text(p) for p in paragraphs if len(p.split()) >= 5]\n",
    "    text = \"\\n\".join(paragraphs)\n",
    "\n",
    "    return {\"source\": \"Forbes\", \"url\": url, \"title\": title, \"date\": date_str, \"text\": text}\n",
    "\n",
    "# --------- Orchestration ----------\n",
    "\n",
    "def main():\n",
    "    rows = []\n",
    "\n",
    "    # 1) collecter les liens\n",
    "    oms_links = list_oms_articles(MAX_ARTICLES_PER_SOURCE)\n",
    "    print(f\"OMS → {len(oms_links)} liens\")\n",
    "\n",
    "    forbes_links = list_forbes_articles(MAX_ARTICLES_PER_SOURCE)\n",
    "    print(f\"Forbes → {len(forbes_links)} liens\")\n",
    "\n",
    "    # 2) parser OMS\n",
    "    for u in tqdm(oms_links, desc=\"OMS - Extraction\"):\n",
    "        row = parse_oms_article(u)\n",
    "        time.sleep(0.8 + random.random() * 0.4)  # respect du site\n",
    "        if row and row.get(\"text\"):\n",
    "            rows.append(row)\n",
    "\n",
    "    # 3) parser Forbes\n",
    "    for u in tqdm(forbes_links, desc=\"Forbes - Extraction\"):\n",
    "        row = parse_forbes_article(u)\n",
    "        time.sleep(0.8 + random.random() * 0.4)\n",
    "        if row and row.get(\"text\"):\n",
    "            rows.append(row)\n",
    "\n",
    "    # 4) consolidation\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        print(\"⚠️ Aucun article valide n'a été extrait.\")\n",
    "        return\n",
    "\n",
    "    # nettoyage léger + filtrage\n",
    "    df[\"title\"] = df[\"title\"].fillna(\"\").map(clean_text)\n",
    "    df[\"text\"] = df[\"text\"].fillna(\"\").map(clean_text)\n",
    "    df = df[df[\"text\"].str.len() > 100]\n",
    "    df = df.drop_duplicates(subset=[\"url\"]).reset_index(drop=True)\n",
    "\n",
    "    # 5) export\n",
    "    df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "    print(f\"✅ Exporté: {OUTPUT_CSV} ({len(df)} articles)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

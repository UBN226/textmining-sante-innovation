{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e33fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pipeline minimal pour le projet \"Santé et Innovation\" (OMS + Forbes Afrique).\n",
    "- Récupère des articles (exemples d'URLs ou via newspaper3k)\n",
    "- Pré-traitement (lower, punctuation, stopwords, tokenization, lemmatize optionnel)\n",
    "- TF-IDF vectorization\n",
    "- Similarité Cosine entre sources\n",
    "- Sentiment (optionnel / basic)\n",
    "- Clustering simple (KMeans) ou topic modeling (NMF)\n",
    "\"\"\"\n",
    "\n",
    "from newspaper import Article\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import NMF\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIG ---\n",
    "# Liste initiale d'URLs (exemples) : tu peux remplir/étendre avec des pages WHO et Forbes Africa\n",
    "URLS = [\n",
    "    # exemples (remplace par les urls réelles d'articles WHO et Forbes Afrique)\n",
    "    \"https://www.who.int/news/item/2025-01-10-example-article\",   # remplacer\n",
    "    \"https://www.forbesafrica.com/technology/2025/01/05/example-article\"  # remplacer\n",
    "]\n",
    "\n",
    "LANG = \"fr\"   # \"fr\" si la majorité du contenu est français, sinon \"en\" ou détecter automatiquement\n",
    "USE_SPACY_LEMMA = True\n",
    "\n",
    "# --- Init ressources linguistiques ---\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = set(stopwords.words('french' if LANG.startswith('fr') else 'english'))\n",
    "# spaCy (fr) si disponible\n",
    "if USE_SPACY_LEMMA:\n",
    "    try:\n",
    "        nlp = spacy.load(\"fr_core_news_sm\") if LANG.startswith('fr') else spacy.load(\"en_core_web_sm\")\n",
    "    except Exception as e:\n",
    "        print(\"spaCy model non trouvé. Désactiver la lemmatisation ou installer le modèle spaCy.\")\n",
    "        USE_SPACY_LEMMA = False\n",
    "        nlp = None\n",
    "\n",
    "# --- Fonctions utilitaires ---\n",
    "def fetch_article(url):\n",
    "    \"\"\"\n",
    "    Tente d'extraire le texte et le titre de l'article.\n",
    "    Utilise newspaper3k si possible, sinon fallback to requests+bs4.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        art = Article(url, language=LANG)\n",
    "        art.download()\n",
    "        art.parse()\n",
    "        return {\"url\": url, \"title\": art.title, \"text\": art.text}\n",
    "    except Exception:\n",
    "        # fallback: simple requests + bs4\n",
    "        try:\n",
    "            res = requests.get(url, timeout=10, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "            # heuristique : récupérer <article> ou concatérer <p>\n",
    "            article_tag = soup.find(\"article\")\n",
    "            if article_tag:\n",
    "                txt = \" \".join([p.get_text(separator=\" \", strip=True) for p in article_tag.find_all(\"p\")])\n",
    "            else:\n",
    "                txt = \" \".join([p.get_text(separator=\" \", strip=True) for p in soup.find_all(\"p\")])\n",
    "            title = soup.title.string if soup.title else \"\"\n",
    "            return {\"url\": url, \"title\": title, \"text\": txt}\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur fetch {url} : {e}\")\n",
    "            return {\"url\": url, \"title\": \"\", \"text\": \"\"}\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Lower, remove accents, punctuation, digits, extra spaces.\"\"\"\n",
    "    if not text: \n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    # remove accents\n",
    "    text = unidecode(text)\n",
    "    # remove punctuation and digits (keep simple letters and spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    # collapse whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_and_clean(text, stopwords_set=STOPWORDS, lemmatize=False):\n",
    "    \"\"\"Tokenize, remove stopwords, optionally lemmatize via spaCy.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha() and len(t) > 1]  # alphabetic tokens\n",
    "    tokens = [t for t in tokens if t not in stopwords_set]\n",
    "    if lemmatize and USE_SPACY_LEMMA and nlp is not None:\n",
    "        doc = nlp(\" \".join(tokens))\n",
    "        tokens = [token.lemma_ for token in doc]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# --- Pipeline principal ---\n",
    "def build_corpus(urls):\n",
    "    rows = []\n",
    "    for u in tqdm(urls, desc=\"Téléchargement articles\"):\n",
    "        art = fetch_article(u)\n",
    "        rows.append(art)\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    df[\"text_raw\"] = df[\"text\"].fillna(\"\")\n",
    "    df[\"text_norm\"] = df[\"text_raw\"].apply(normalize_text)\n",
    "    df[\"text_clean\"] = df[\"text_norm\"].apply(lambda t: tokenize_and_clean(t, STOPWORDS, lemmatize=USE_SPACY_LEMMA))\n",
    "    return df\n",
    "\n",
    "def vectorize_tfidf(corpus, max_features=5000, ngram_range=(1,2)):\n",
    "    \"\"\"Retourne matrice TF-IDF (sparse) et le vectorizer.\"\"\"\n",
    "    vec = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
    "    X = vec.fit_transform(corpus)\n",
    "    return X, vec\n",
    "\n",
    "def compute_similarity(X):\n",
    "    \"\"\"Matrice de similarité cosinus (N x N).\"\"\"\n",
    "    return cosine_similarity(X)\n",
    "\n",
    "def sentiment_basic(text):\n",
    "    \"\"\"\n",
    "    Version basique de sentiment : heuristique par lexique (très simple).\n",
    "    Pour du sérieux -> utiliser transformers pipeline multilingue ou model francophone.\n",
    "    \"\"\"\n",
    "    # liste minimale d'exemples\n",
    "    pos_words = {\"bon\",\"positif\",\"amelior\",\"utile\",\"important\",\"prometteur\"}\n",
    "    neg_words = {\"risque\",\"mort\",\"critique\",\"probleme\",\"negatif\",\"alerte\",\"grave\"}\n",
    "    tset = set(text.split())\n",
    "    score = sum(1 for w in tset if w in pos_words) - sum(1 for w in tset if w in neg_words)\n",
    "    return score\n",
    "\n",
    "# --- Exécution exemple ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) construire corpus\n",
    "    df = build_corpus(URLS)\n",
    "    print(f\"{len(df)} articles récupérés.\")\n",
    "\n",
    "    # 2) pré-traitement\n",
    "    df = preprocess_dataframe(df)\n",
    "    df[\"sentiment_score\"] = df[\"text_clean\"].apply(sentiment_basic)\n",
    "\n",
    "    # 3) TF-IDF\n",
    "    X, tfidf = vectorize_tfidf(df[\"text_clean\"].fillna(\"\"), max_features=3000, ngram_range=(1,2))\n",
    "    sim = compute_similarity(X)\n",
    "\n",
    "    # 4) similarité moyenne OMS-vs-Forbes (exemple : on marque la source dans df)\n",
    "    # Ici tu dois fournir un champ 'source' (OMS/Forbes) dans ton df\n",
    "    # Exemple : df['source'] = ['OMS','Forbes', ...]\n",
    "    if 'source' in df.columns:\n",
    "        sources = df['source'].values\n",
    "        # calcule similarité moyenne entre groups\n",
    "        import numpy as np\n",
    "        mask_oms = (sources == 'OMS')\n",
    "        mask_forbes = (sources == 'Forbes')\n",
    "        if mask_oms.sum() and mask_forbes.sum():\n",
    "            sub = sim[np.ix_(mask_oms, mask_forbes)]\n",
    "            print(\"Similarité moyenne OMS<->Forbes:\", sub.mean())\n",
    "\n",
    "    # 5) clustering (KMeans) sur TF-IDF\n",
    "    k = 5\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    df[\"cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "    # 6) topic modeling NMF (optionnel)\n",
    "    nmf_k = 6\n",
    "    nmf = NMF(n_components=nmf_k, random_state=42)\n",
    "    W = nmf.fit_transform(X)\n",
    "    H = nmf.components_\n",
    "    # top mots par topic\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        top_features_ind = topic.argsort()[:-11:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        topics.append(top_features)\n",
    "        print(f\"Topic {topic_idx} : {', '.join(top_features)}\")\n",
    "\n",
    "    # 7) sauvegarde\n",
    "    df.to_csv(\"articles_processed.csv\", index=False)\n",
    "    print(\"Pipeline terminé. Fichier articles_processed.csv créé.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
